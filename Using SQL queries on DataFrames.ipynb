{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL queries on DataFrames\n",
    "\n",
    "- - -\n",
    "\n",
    "#### Declarative Manipulation (SQL)\n",
    "* Advantage: You need to describe only **what** is the result you want.\n",
    "* Disadvantage: SQL does not have primitives for common analysis operations such as **covariance**\n",
    "\n",
    "\n",
    "- - -\n",
    "\n",
    "* Spark supports a subset of the Hive SQL query language.\n",
    "* For example, You can use Hive select syntax to select a subset of the rows in a dataframe.\n",
    "* To use sql on a dataframe you need to ``first register it as a TempTable``\n",
    "* For variety, we are using here a small dataframe loaded from a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x7fb6877e7b00>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1TZyM7Gfc6XWLot-L36TDV-JwySgHxGv4\", \"people.json\")\n",
    "data_file = \"./people.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(data_file)\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method~\n",
    "people.show()\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ★Register the DataFrame as a table★\n",
    "\n",
    "* **Registers a DataFrame as a Temporary Table in the SQLContext**\n",
    "* Usage : ``registerTempTable(tableName)``\n",
    "* Arguments :\n",
    "```\n",
    "x: A SparkSQL DataFrame, \n",
    "talbeName : A character vector containing the name of the table\n",
    "```\n",
    "* Example : [DataFrame_Spark].registerTempTable(\"hi_hi_hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show를 사용하면....\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "\n",
      "collect를 사용하면....\n",
      "[Row(name='Justin')]\n"
     ]
    }
   ],
   "source": [
    "## Register this DataFrame as a table\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "## SQL statements can be run by using the sql methods provided by sqlContext\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# show를 사용했을 때, DataFrame return\n",
    "print(\"show를 사용하면....\")\n",
    "teenagers.show()\n",
    "\n",
    "# collect를 사용했을 때, Row형태의 RDD return\n",
    "print(\"\\ncollect를 사용하면....\")\n",
    "print(teenagers.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1 - lec6 복습!!(20 point)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "* **출력**과 같은 값이 나오도록 code를 작성하세요 (개당 4 point)\n",
    "- - -\n",
    "**출력**\n",
    "\n",
    "```\n",
    "1 ====>  [Row(name='Justin')]\n",
    "2 ====>  Justin\n",
    "3 ====>  Row(name='Justin')\n",
    "4 ====> \n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n",
    "\n",
    "5 ====> \n",
    "+---+------+\n",
    "|age|  name|\n",
    "+---+------+\n",
    "| 30|  Andy|\n",
    "| 19|Justin|\n",
    "+---+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ====>  [Row(name='Justin')]\n",
      "2 ====>  Justin\n",
      "3 ====>  Row(name='Justin')\n",
      "4 ====> \n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "5 ====> \n",
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "'''\n",
    "print('1 ====> ', ?)\n",
    "print('2 ====> ', ?)\n",
    "print('3 ====> ', ?)\n",
    "print('4 ====> ', ?)\n",
    "print(\"5 ====> \", ?) \n",
    "'''\n",
    "\n",
    "# solution\n",
    "print('1 ====> ', teenagers.collect())\n",
    "print('2 ====> ',teenagers.collect()[0]['name'])\n",
    "print('3 ====> ', teenagers.collect()[0])\n",
    "print(\"4 ====> \") \n",
    "people.filter(col(\"age\") == 30).show()\n",
    "print(\"5 ====> \") \n",
    "people.filter(col(\"age\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Basic\n",
    "\n",
    "---\n",
    "\n",
    "SQL에 대해 기본적인 사용법을 학습합니다.\n",
    "\n",
    "* SQL 구문\n",
    "```\n",
    "SELECT [열] \n",
    "FROM [테이블] \n",
    "WHERE [조건]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1hAHV6vC6FvVgrYnoN-lR-IfH488-H121 into ./NY.tgz... Done.\n",
      "NY.parquet/\n",
      "NY.parquet/_SUCCESS\n",
      "NY.parquet/part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# NY weather download & register\n",
    "from os.path import exists\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "if exists(\"./NY.tgz\"):\n",
    "    !rm -rf ./NY.tgz\n",
    "if exists(\"./NY.parquet\"):\n",
    "    !rm -rf ./NY.parquet\n",
    "    \n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = './NY.tgz')\n",
    "!tar -xzvf NY.tgz\n",
    "\n",
    "# read & register\n",
    "df = sqlContext.read.load(\"NY.parquet\")\n",
    "df.registerTempTable(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) DESC\n",
    "* 테이블 구조 확인\n",
    "* ``[DataFrame].printSchema``와 동일한 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|    Station|   string|   null|\n",
      "|Measurement|   string|   null|\n",
      "|       Year|   bigint|   null|\n",
      "|     Values|   binary|   null|\n",
      "| dist_coast|   double|   null|\n",
      "|   latitude|   double|   null|\n",
      "|  longitude|   double|   null|\n",
      "|  elevation|   double|   null|\n",
      "|      state|   string|   null|\n",
      "|       name|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.printSchema()\n",
    "\n",
    "# SQL code\n",
    "query = \"\"\"DESC weather\"\"\"\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) FIRST와 LAST\n",
    "* ``FIRST`` : 첫 번째 ROW\n",
    "* ``LAST`` : 마지막 ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "from pyspark.sql.functions import col, first, last\n",
    "df.select(first(col(\"Station\")), last(col(\"Station\"))).show()\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT FIRST(Station), LAST(Station) FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) SELECT와 AS\n",
    "* ``SELECT`` : 특정 column 선택\n",
    "* ``AS``     : 특정 column의 이름 변경\n",
    "* 사용팁 : ``select``와 동시에 ``as``를 이용하여 column의 이름을 변경한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|JINOO_Station|          name_JW|\n",
      "+-------------+-----------------+\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+-------------+-----------------+\n",
      "|JINOO_Station|          name_JW|\n",
      "+-------------+-----------------+\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.select(col(\"Station\").alias(\"JINOO_Station\"), col(\"name\").alias(\"name_JW\")).show(3)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT Station AS JINOO_Station, name AS name_JW FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) SELECT와 DISTINCT\n",
    "* ``SELECT`` : 특정 column 선택\n",
    "* ``DISTICNT``     : 결과값에 중복된 데이터가 있으면 중복되는 N개의 데이터는 1건으로 처리해서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|JINOO_Station_DISTINCT|             name_JW|\n",
      "+----------------------+--------------------+\n",
      "|           USC00300505|       BEAVER MEADOW|\n",
      "|           USC00305426|         MOHONK LAKE|\n",
      "|           USC00305798|NEW YORK BENSONHURST|\n",
      "+----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+----------------------+--------------------+\n",
      "|JINOO_Station_DISTINCT|             name_JW|\n",
      "+----------------------+--------------------+\n",
      "|           USC00300505|       BEAVER MEADOW|\n",
      "|           USC00305426|         MOHONK LAKE|\n",
      "|           USC00305798|NEW YORK BENSONHURST|\n",
      "+----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.select(col(\"Station\").alias(\"JINOO_Station_DISTINCT\"), col(\"name\").alias(\"name_JW\")).distinct().show(3)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT DISTINCT Station AS JINOO_Station_DISTINCT, name AS name_JW FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) COUNT, AVG, SUM, MAX, MIN\n",
    "* 이름 그대로의 역할!!\n",
    "* SELECT와 연계하여 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|   avg(dist_coast)|    sum(dist_coast)|\n",
      "+------------------+-------------------+\n",
      "|245.78455113006692|4.138962684120101E7|\n",
      "+------------------+-------------------+\n",
      "\n",
      "+------------------+-----------------+\n",
      "|    min(elevation)|   max(elevation)|\n",
      "+------------------+-----------------+\n",
      "|-999.9000244140625|838.2000122070312|\n",
      "+------------------+-----------------+\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|   avg(dist_coast)|    sum(dist_coast)|\n",
      "+------------------+-------------------+\n",
      "|245.78455113006692|4.138962684120101E7|\n",
      "+------------------+-------------------+\n",
      "\n",
      "+------------------+-----------------+\n",
      "|    min(elevation)|   max(elevation)|\n",
      "+------------------+-----------------+\n",
      "|-999.9000244140625|838.2000122070312|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "from pyspark.sql.functions import count, avg, sum, max, min\n",
    "df.select(count(col(\"Station\"))).show()\n",
    "df.select(avg(col(\"dist_coast\")), sum(col(\"dist_coast\"))).show()\n",
    "df.select(min(col(\"elevation\")), max(col(\"elevation\"))).show()\n",
    "\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query1 = \"\"\"SELECT COUNT(Station) FROM weather\"\"\"\n",
    "query2 = \"\"\"SELECT AVG(dist_coast), SUM(dist_coast) FROM weather\"\"\"\n",
    "query3 = \"\"\"SELECT MIN(elevation), MAX(elevation) FROM weather\"\"\"\n",
    "\n",
    "sqlContext.sql(query1).show(3)\n",
    "sqlContext.sql(query2).show(3)\n",
    "sqlContext.sql(query3).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) WHERE\n",
    "* **조건절** \n",
    "* pyspark의 ``filter``와 동일한 기능\n",
    "* example\n",
    "\n",
    "```\n",
    "# Example 1: weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\n",
    "SELECT DISTINCT * FROM weather WHERE elevation >= 800\n",
    "\n",
    "# Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\n",
    "SELECT Station FROM weather WHERE elevation < 100\n",
    "\n",
    "# Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\"\n",
    "SELECT DISTINCT * FROM weather WHERE elevation >= 600 AND elevation < 800\n",
    "\n",
    "```\n",
    "\n",
    "* example 외에도 사용법이 너무 많습니다. [눌러1](https://loveiskey.tistory.com/58), [눌러2](http://www.sqlprogram.com/Basics/sql-where.aspx), [눌러3](https://www.guru99.com/where-clause.html)을 참고해주세요.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 : weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|          latitude|         longitude|        elevation|state|       name|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "|USC00307799|   TMAX_s20|2005|[02 C9 8F C9 0C C...| 128.7790069580078|42.016700744628906|-74.41670227050781|807.7000122070312|   NY|  SLIDE MTN|\n",
      "|USC00304520|       SNOW|1922|[00 00 00 00 00 0...|285.82501220703125|44.099998474121094|             -74.0|838.2000122070312|   NY|LAKE COLDEN|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\n",
      "+-----------+\n",
      "|    Station|\n",
      "+-----------+\n",
      "|USC00309055|\n",
      "|USC00309055|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|          latitude|         longitude|        elevation|state|        name|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "|USC00303124|   PRCP_s20|1993|[CE 4F C2 4F B0 4...|   349.06201171875| 42.70000076293945| -77.4000015258789|            602.0|   NY|GANNETT HILL|\n",
      "|USC00302366|   PRCP_s20|2001|[CA 50 B2 50 9B 5...|141.85299682617188|42.235599517822266|-74.14330291748047|606.9000244140625|   NY| EAST JEWETT|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL code\n",
    "print(\"Example 1 : weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\")\n",
    "query1 = \"\"\"SELECT DISTINCT * FROM weather WHERE elevation >= 800\"\"\"\n",
    "sqlContext.sql(query1).show(2)\n",
    "\n",
    "print(\"Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\")\n",
    "query2 = \"\"\"SELECT Station FROM weather WHERE elevation < 100\"\"\"\n",
    "sqlContext.sql(query2).show(2)\n",
    "\n",
    "print(\"Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\")\n",
    "query3 = \"\"\"SELECT DISTINCT * FROM weather WHERE elevation >= 600 AND elevation < 800\"\"\"\n",
    "sqlContext.sql(query3).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) GROUP BY와 HAVING\n",
    "* GROUP BY문은 동일한 값을 가진 데이터를 집계해서 조회하고자 할 때 사용하는 문장이다.\n",
    "* 작성 방법\n",
    "\n",
    "```\n",
    "SELECT  [구문]\n",
    "FROM    [구문]\n",
    "GROUP BY [구문]\n",
    "HAVING  [구문]\n",
    "\n",
    "- 집계할 컬럼을 GROUP BY절 뒤에 적어준다.\n",
    "- SELECT절에는 GROUP BY에 명시된 컬럼만 사용할 수 있다.\n",
    "- HAVING에는 GROUP BY에 조건을 추가한다\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+\n",
      "|Measurement|count|MinYear|\n",
      "+-----------+-----+-------+\n",
      "|   TMIN_s20|13442|   1873|\n",
      "|       TMIN|13442|   1873|\n",
      "|   SNOW_s20|15629|   1884|\n",
      "|       TOBS|10956|   1876|\n",
      "|   SNWD_s20|14617|   1888|\n",
      "+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+-----------+-----+-------+\n",
      "|Measurement|count|MinYear|\n",
      "+-----------+-----+-------+\n",
      "|   TMIN_s20|13442|   1873|\n",
      "|       TMIN|13442|   1873|\n",
      "|   SNOW_s20|15629|   1884|\n",
      "|       TOBS|10956|   1876|\n",
      "|   SNWD_s20|14617|   1888|\n",
      "+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.groupby(col(\"Measurement\")).agg({\"Measurement\" : \"count\", \"year\": \"min\"})\\\n",
    "  .withColumnRenamed(\"count(Measurement)\", \"count\")\\\n",
    "  .withColumnRenamed(\"min(year)\", \"MinYear\")\\\n",
    "  .show(5)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT Measurement, COUNT(Measurement) AS count, MIN(year) AS MinYear\n",
    "           FROM weather\n",
    "           GROUP BY Measurement\"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|2011| 265.4277518365087|253.01556445820043|\n",
      "|2010|262.55130806776833|248.06859466340276|\n",
      "|2009| 261.5488291051326|246.52965264467915|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|2011| 265.4277518365087|253.01556445820043|\n",
      "|2010|262.55130806776833|248.06859466340276|\n",
      "|2009| 261.5488291051326|246.52965264467915|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year를 GROUP BY를 통해 그룹화 하고, year 별 dist_coast와 elevation의 평균을 구하고\n",
    "# year를 기준으로 오름차순 한다.\n",
    "from pyspark.sql.functions import desc\n",
    "df.groupby(col(\"year\")).agg({\"dist_coast\":\"avg\",\"elevation\":\"avg\"})\\\n",
    "  .sort(desc(\"year\")).show(5)\n",
    "\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT year, AVG(elevation), AVG(dist_coast)\n",
    "           FROM weather\n",
    "           GROUP BY year\n",
    "           ORDER BY year DESC\n",
    "           \"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★★★In SQL★★★★★★\n",
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|1915| 264.1221125600379|258.39760847437293|\n",
      "|1914|262.33999927043914| 259.3964216718078|\n",
      "|1913|271.95626452024027| 257.1068253879962|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL code\n",
    "# HAVING 추가\n",
    "# year를 GROUP BY를 통해 그룹화 하고, year 별 dist_coast와 elevation의 평균을 구하고\n",
    "# dist_coast의 평균이 256 이상이고 elevation의 평균이 260 이상인 ROW를 구하되\n",
    "# year를 기준으로 오름차순 한다.\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT year, AVG(elevation), AVG(dist_coast)\n",
    "           FROM weather\n",
    "           GROUP BY year\n",
    "           HAVING AVG(dist_coast) >= 256 AND AVG(elevation) >= 260\n",
    "           ORDER BY year DESC\n",
    "           \"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★★★★WHERE와 HAVING???★★★★\n",
    "\n",
    "query에 `WHERE`와 `HAVING`이 있다면, `WHERE`가 적용된 후*(`조건이 적용된 table`에)* `HAVING`이 적용된다.\n",
    "\n",
    "* **반드시 다음 주소에 접속하여 차이점을 확인할 것 (2와 3).**\n",
    "[http://wiki.gurubee.net/pages/viewpage.action?pageId=26743892](http://wiki.gurubee.net/pages/viewpage.action?pageId=26743892)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2 - GROUP BY와 HAVING 적용해보자! (30 point)\n",
    "\n",
    "- - -\n",
    "task 별로 ``GROUP BY``와 위에서 학습한 여러 ``SQL function``을 이용하여 query 를 작성하세요(task 별 10 point)\n",
    "\n",
    "---\n",
    "\n",
    "**task**\n",
    "\n",
    "* 1 : ``weather``에서 ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 가장 최근 ``year``의 이름은 ``maxYear``, 평균 ``dist_coast``와 ``elevation``의 이름은 ``avgDist_coast``, ``avgElevation``으로 변경하세요.(10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 2 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 3 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구합니다. 그리고 ``name``별 평균 ``dist_coast``가 175.44400024414062이거나 평균 ``elevation``이 304.5인 결과를 출력하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)\n",
    "\n",
    "- - -\n",
    "**출력 예시**\n",
    "```\n",
    "# task1 output\n",
    "+-------------+-------+------------------+-----------------+\n",
    "|         name|maxYear|     avgDist_coast|     avgElevation|\n",
    "+-------------+-------+------------------+-----------------+\n",
    "|    ADAMS CTR|   1950|  376.802001953125|121.9000015258789|\n",
    "|      ADDISON|   2013| 296.1679992675781|            304.5|\n",
    "|ADDISON 1 NNE|   1988| 300.0060119628906|373.3999938964844|\n",
    "|       ALBANY|   1970|166.00100708007812|              0.0|\n",
    "|    ALBANY AP|   2013|175.44400024414062| 95.0999984741211|\n",
    "+-------------+-------+------------------+-----------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "# task2 output\n",
    "+-----------+-------+------------------+------------------+\n",
    "|       name|maxYear|     avgDist_coast|      avgElevation|\n",
    "+-----------+-------+------------------+------------------+\n",
    "|    ADDISON|   2013| 296.1679992675781|             304.5|\n",
    "|  ALBANY AP|   2013|175.44400024414062|  95.0999984741211|\n",
    "|ALBION 2 NE|   2012| 436.3030090332031|134.10000610351562|\n",
    "| ALCOVE DAM|   2013|152.88999938964844|             185.0|\n",
    "|     ALFRED|   2013| 330.6700134277344|             520.0|\n",
    "+-----------+-------+------------------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "# task3 output\n",
    "+---------+-------+------------------+----------------+\n",
    "|     name|maxYear|     avgDist_coast|    avgElevation|\n",
    "+---------+-------+------------------+----------------+\n",
    "|ALBANY AP|   2013|175.44400024414062|95.0999984741211|\n",
    "|  ADDISON|   2013| 296.1679992675781|           304.5|\n",
    "+---------+-------+------------------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 1 : ``weather``에서 ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 가장 최근 ``year``의 이름은 ``maxYear``, 평균 ``dist_coast``와 ``elevation``의 이름은 ``avgDist_coast``, ``avgElevation``으로 변경하세요.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+------------------+-----------------+\n",
      "|         name|maxYear|     avgDist_coast|     avgElevation|\n",
      "+-------------+-------+------------------+-----------------+\n",
      "|    ADAMS CTR|   1950|  376.802001953125|121.9000015258789|\n",
      "|      ADDISON|   2013| 296.1679992675781|            304.5|\n",
      "|ADDISON 1 NNE|   1988| 300.0060119628906|373.3999938964844|\n",
      "|       ALBANY|   1970|166.00100708007812|              0.0|\n",
      "|    ALBANY AP|   2013|175.44400024414062| 95.0999984741211|\n",
      "+-------------+-------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-1 답 작성\n",
    "task1_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 GROUP BY name\n",
    "                 ORDER BY name\n",
    "\"\"\"\n",
    "# output\n",
    "sqlContext.sql(task1_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 2 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+------------------+\n",
      "|       name|maxYear|     avgDist_coast|      avgElevation|\n",
      "+-----------+-------+------------------+------------------+\n",
      "|    ADDISON|   2013| 296.1679992675781|             304.5|\n",
      "|  ALBANY AP|   2013|175.44400024414062|  95.0999984741211|\n",
      "|ALBION 2 NE|   2012| 436.3030090332031|134.10000610351562|\n",
      "| ALCOVE DAM|   2013|152.88999938964844|             185.0|\n",
      "|     ALFRED|   2013| 330.6700134277344|             520.0|\n",
      "+-----------+-------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-2 답 작성\n",
    "task2_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 WHERE year >= 2000\n",
    "                 GROUP BY name\n",
    "                 ORDER BY name\n",
    "\"\"\"\n",
    "# output\n",
    "sqlContext.sql(task2_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 3 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구합니다. 그리고 ``name``별 평균 ``dist_coast``가 175.44400024414062이거나 평균 ``elevation``이 304.5인 결과를 출력하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------+----------------+\n",
      "|     name|maxYear|     avgDist_coast|    avgElevation|\n",
      "+---------+-------+------------------+----------------+\n",
      "|ALBANY AP|   2013|175.44400024414062|95.0999984741211|\n",
      "|  ADDISON|   2013| 296.1679992675781|           304.5|\n",
      "+---------+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-3 답 작성\n",
    "task3_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 WHERE year >= 2000\n",
    "                 GROUP BY name\n",
    "                 HAVING avgElevation == 304.5 OR avgDist_coast == 175.44400024414062\n",
    "\"\"\"\n",
    "# output\n",
    "sqlContext.sql(task3_query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \n",
    "\n",
    "### DataFrame과 SQL을 사용하여 HW4 Exercise 3 다시 풀기! (25 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "\n",
    "**위의 데이터는 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv와 pre.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "- 1. sqlContext.read.csv를 이용하여 ``regular.csv``를 DataFrame으로 만든 후 . ``registerTempTable(\"task1_table\")``을 이용하여 table로 등록합니다. 그리고 SQL의 ``DESC``를 활용하여 구조를 확인합니다(5 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. task1에서 생성된 ``task1_table``에서, 타율(avg) column의 type을 String에서 Double로 변환 후 ``registerTempTable(\"task2_table\")``을 사용하여 새로운 table로 등록합니다(10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 3. task2에서 생성된 ``task2_table``에서 타율(avg)이 0.300을 초과하는 모든 선수에 대해(중복포함), **각 타자의 3할을 친 횟수가 8 이상인 타자의 이름(batter_name)과 3할을 친 횟수(avgCount)를 구합니다**. 이에 덧붙여서, 3할을 친 횟수를 기준으로 내림차순을 적용하는 query를 작성하고 출력합니다.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from os.path import exists\n",
    "\n",
    "if exists(\"./regular.csv\"):\n",
    "    !rm -rf regular.csv\n",
    "\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1t3icaDgI5KeNEwNmaWFOYGYQtdY8NOMm\",\n",
    "              \"regular.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ★★★ table 관련해서 Error 발생시 ★★★\n",
    "\n",
    "아래의 코드를 이용해서, 등록 table을 지운 후 진행합시다.!\n",
    "\n",
    "```\n",
    "sqlContext.sql(\"SHOW tables\").show()\n",
    "sqlContext.dropTempTable(\"[지우고자 하는 table 이름]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "\n",
    "- 1. sqlContext.read.csv를 이용하여 ``regular.csv``를 DataFrame으로 만든 후 . ``registerTempTable(\"regular_table\")``을 이용하여 table로 등록합니다. 그리고 SQL의 ``DESC``를 활용하여 구조를 확인합니다. (5 point)\n",
    "\n",
    "```\n",
    "# task1 output\n",
    "+-----------+---------+-------+\n",
    "|   col_name|data_type|comment|\n",
    "+-----------+---------+-------+\n",
    "|  batter_id|   string|   null|\n",
    "|batter_name|   string|   null|\n",
    "|       year|   string|   null|\n",
    "|       team|   string|   null|\n",
    "|        avg|   string|   null|\n",
    "|          G|   string|   null|\n",
    "|         AB|   string|   null|\n",
    "|          R|   string|   null|\n",
    "|          H|   string|   null|\n",
    "|         2B|   string|   null|\n",
    "|         3B|   string|   null|\n",
    "|         HR|   string|   null|\n",
    "|         TB|   string|   null|\n",
    "|        RBI|   string|   null|\n",
    "|         SB|   string|   null|\n",
    "|         CS|   string|   null|\n",
    "|         BB|   string|   null|\n",
    "|        HBP|   string|   null|\n",
    "|         SO|   string|   null|\n",
    "|        GDP|   string|   null|\n",
    "+-----------+---------+-------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|  batter_id|   string|   null|\n",
      "|batter_name|   string|   null|\n",
      "|       year|   string|   null|\n",
      "|       team|   string|   null|\n",
      "|        avg|   string|   null|\n",
      "|          G|   string|   null|\n",
      "|         AB|   string|   null|\n",
      "|          R|   string|   null|\n",
      "|          H|   string|   null|\n",
      "|         2B|   string|   null|\n",
      "|         3B|   string|   null|\n",
      "|         HR|   string|   null|\n",
      "|         TB|   string|   null|\n",
      "|        RBI|   string|   null|\n",
      "|         SB|   string|   null|\n",
      "|         CS|   string|   null|\n",
      "|         BB|   string|   null|\n",
      "|        HBP|   string|   null|\n",
      "|         SO|   string|   null|\n",
      "|        GDP|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3-1 답작성\n",
    "sqlContext.read.csv(\"regular.csv\", header = True).registerTempTable(\"task1_table\")\n",
    "sqlContext.sql(\"DESC task1_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 2. task1에서 생성된 ``task1_table``에서, 타율(avg) column의 type을 String에서 Double로 변환 후 ``registerTempTable(\"task2_table\")``을 사용하여 새로운 table로 등록합니다(10 point)\n",
    "\n",
    "```\n",
    "# SQL에서 column type 변경하기\n",
    "SELECT *, cast([column 이름] as [바꿀 type]) AS [새로운 column 이름] FROM [table]\n",
    "# 여기서 기존의 column은 제거되지 않고 type이 변경된 column이 추가됩니다.\n",
    "\n",
    "# output\n",
    "Row(col_name='avg_double', data_type='double', comment=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col_name='avg_double', data_type='double', comment=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-2 답 작성\n",
    "query2 = \"\"\"SELECT *, cast(avg as double) AS avg_double FROM task1_table\"\"\"\n",
    "sqlContext.sql(query2).registerTempTable(\"task2_table\")\n",
    "\n",
    "# output\n",
    "sqlContext.sql(\"DESC task2_table\").collect()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task 3**\n",
    "- 3. task2에서 생성된 ``task2_table``에서 타율(avg)이 0.300을 초과하는 모든 선수에 대해(중복포함), **각 타자의 3할을 친 횟수가 8 이상인 타자의 이름(batter_name)과 3할을 친 횟수(avgCount)를 구합니다**. 이에 덧붙여서, 3할을 친 횟수를 기준으로 내림차순을 적용하는 query를 작성하고 출력합니다.(10 point)\n",
    "\n",
    "```\n",
    "+-----------+--------+\n",
    "|batter_name|avgCount|\n",
    "+-----------+--------+\n",
    "|     김태균|      13|\n",
    "|     김주찬|      11|\n",
    "|     이진영|      11|\n",
    "|     이택근|      10|\n",
    "|     손아섭|      10|\n",
    "|     정근우|       9|\n",
    "|     장성호|       9|\n",
    "|     박용택|       9|\n",
    "|     김동주|       9|\n",
    "|     정성훈|       8|\n",
    "|     이대호|       8|\n",
    "|     최형우|       8|\n",
    "|     박한이|       8|\n",
    "|     김현수|       8|\n",
    "+-----------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|batter_name|avgCount|\n",
      "+-----------+--------+\n",
      "|     김태균|      13|\n",
      "|     이진영|      11|\n",
      "|     김주찬|      11|\n",
      "|     이택근|      10|\n",
      "|     손아섭|      10|\n",
      "|     장성호|       9|\n",
      "|     김동주|       9|\n",
      "|     정근우|       9|\n",
      "|     박용택|       9|\n",
      "|     최형우|       8|\n",
      "|     이대호|       8|\n",
      "|     정성훈|       8|\n",
      "|     박한이|       8|\n",
      "|     김현수|       8|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3-3 답 작성\n",
    "query3 = \"\"\"SELECT batter_name, COUNT(avg_double) AS avgCount\n",
    "            FROM task2_table\n",
    "            WHERE avg_double > 0.300\n",
    "            GROUP BY batter_name\n",
    "            HAVING avgCount >= 8\n",
    "            ORDER BY avgCount DESC\n",
    "            \"\"\"\n",
    "# output\n",
    "sqlContext.sql(query3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 \n",
    "\n",
    "### 데이터를 찾고 DataFrame과 SQL을 적용해보자 (25 point)\n",
    "- - -\n",
    "\n",
    "지금까지 계속 똑같은 데이터(야구와 야구 그리고 baseball 등)를 사용하여 과제를 진행하였다. \n",
    "\n",
    "지겹지 않은가??? *(??? : 사실 내가 지겹다.)*\n",
    "\n",
    "<br>\n",
    "\n",
    "HW6 Exercise 4는 야구 데이터의 매너리즘에 빠진 당신을 위해 준비하였다. ㅋ\n",
    "\n",
    "- - -\n",
    "\n",
    "**task**\n",
    "* 1. 공공데이터포털([https://www.data.go.kr/](https://www.data.go.kr/)), 데이콘([https://dacon.io/](https://dacon.io/)), kaggle([https://www.kaggle.com/](https://www.kaggle.com/)]) 등 각종 데이터를 제공하는 사이트에 접속하여 데이터를 찾습니다. 데이터의 형식은 상관 X. ``하지만, 우리가 학습해본 csv, json, parquet가 편하지 않을까...`` **데이터를 찾은 후 `sqlContext의 read.[형식에 맞게]`를 적용하여 DataFrame으로 저장한다**. 그 후, ``registerTempTable``을 사용하여 ``table``로 등록후, SQL을 사용하여 데이터의 구조를 출력합니다. (10 point)\n",
    "\n",
    "    1. **[DataFrameReader 참고](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)**\n",
    "\n",
    "<br>\n",
    "\n",
    " \n",
    "* 2. task1에서 불러온 데이터에 대해 설명하시오. (5 point)\n",
    "\n",
    "    **<반드시 포함되어야할 내용>**\n",
    "    \n",
    "    1. 데이터의 출처, 데이터의 description, 데이터 활용 방안 등\n",
    "    \n",
    "<br>\n",
    "\n",
    "* 3. task1의 데이터를 활용하여 자유롭게 결과를 출력하고, 왜 그러한 결과를 출력했는지와 코드에 대한 설명을 추가하세요.(10 point)\n",
    "\n",
    "    **<반드시 포함되어야할 사항>**\n",
    "    \n",
    "    1. pyspark 함수가 아닌 **SQL 명령어 사용**. 또한, ``GROUP BY``, ``HAVING`` 반드시 포함되어야 함\n",
    "    2. 결과에 대한 설명(print로 작성하시오)\n",
    "    3. 주석 반드시 포함(코드에 대한 주석 #)\n",
    "    \n",
    "- - -\n",
    "**출력 예시**\n",
    "\n",
    "![png](http://www.artinsight.co.kr/data/news/1811/1988088618_t0JncQ9w_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 1. 공공데이터포털([https://www.data.go.kr/](https://www.data.go.kr/)), 데이콘([https://dacon.io/](https://dacon.io/)), kaggle([https://www.kaggle.com/](https://www.kaggle.com/)]) 등 각종 데이터를 제공하는 사이트에 접속하여 데이터를 찾습니다. 데이터의 형식은 상관 X. ``하지만, 우리가 학습해본 csv, json, parquet가 편하지 않을까...`` **데이터를 찾은 후 `sqlContext의 read.[형식에 맞게]`를 적용하여 DataFrame으로 저장한다**. 그 후, ``registerTempTable``을 사용하여 ``table``로 등록후, SQL을 사용하여 데이터의 구조를 출력합니다. (10 point)\n",
    "\n",
    "```\n",
    "sqlContext.read.csv(\"./전국금연구역표준데이터.csv\", header = True, encoding=\"EUC-kR\").registerTempTable(\"non_smoking\")\n",
    "\n",
    "★ task1 example output ★\n",
    "★ tempTable SHOW★\n",
    "+--------+-----------+-----------+\n",
    "|database|  tableName|isTemporary|\n",
    "+--------+-----------+-----------+\n",
    "|        |non_smoking|       true|\n",
    "|        |     people|       true|\n",
    "|        |task1_table|       true|\n",
    "|        |task2_table|       true|\n",
    "|        |    weather|       true|\n",
    "+--------+-----------+-----------+\n",
    "\n",
    "★ DESC★\n",
    "+----------------+---------+-------+\n",
    "|        col_name|data_type|comment|\n",
    "+----------------+---------+-------+\n",
    "|      금연구역명|   string|   null|\n",
    "|금연구역범위상세|   string|   null|\n",
    "|          시도명|   string|   null|\n",
    "|        시군구명|   string|   null|\n",
    "|    금연구역구분|   string|   null|\n",
    "+----------------+---------+-------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★ tempTable SHOW★\n",
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |non_smoking|       true|\n",
      "|        |     people|       true|\n",
      "|        |task1_table|       true|\n",
      "|        |task2_table|       true|\n",
      "|        |    weather|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n",
      "★ DESC★\n",
      "+----------------+---------+-------+\n",
      "|        col_name|data_type|comment|\n",
      "+----------------+---------+-------+\n",
      "|      금연구역명|   string|   null|\n",
      "|금연구역범위상세|   string|   null|\n",
      "|          시도명|   string|   null|\n",
      "|        시군구명|   string|   null|\n",
      "|    금연구역구분|   string|   null|\n",
      "+----------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4-1 답안 작성\n",
    "# example code\n",
    "#sqlContext.read.csv(\"[당신이 찾은 데이터의 path를 입력하세요]\")\n",
    "\n",
    "sqlContext.read.csv(\"./전국금연구역표준데이터.csv\", header = True, encoding=\"EUC-kR\")\\\n",
    ".registerTempTable(\"non_smoking\")\n",
    "\n",
    "\n",
    "# example output\n",
    "# table명은 자유롭게 \n",
    "print(\"★ tempTable SHOW★\")\n",
    "sqlContext.sql(\"SHOW tables\").show()\n",
    "print(\"★ DESC★\")\n",
    "sqlContext.sql(\"DESC non_smoking\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 2. task1에서 불러온 데이터에 대해 설명하시오. (5 point)\n",
    "\n",
    "    **<반드시 포함되어야할 내용>**\n",
    "    \n",
    "    1. 데이터의 출처, 데이터의 description, 데이터 활용 방안 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4-2 답안 작성] \n",
      "\n",
      "데이터의 출처 : [여기에 작성]\n",
      "데이터의 description : [여기에 작성]\n",
      "데이터 활용 방안 : [여기에 작성]\n",
      "기타 등등 : [여기에 작성]\n"
     ]
    }
   ],
   "source": [
    "# 4-2 답안 작성\n",
    "print(\"[4-2 답안 작성] \\n\")\n",
    "print(\"데이터의 출처 : [여기에 작성]\")\n",
    "print(\"데이터의 description : [여기에 작성]\")\n",
    "print(\"데이터 활용 방안 : [여기에 작성]\")\n",
    "print(\"기타 등등 : [여기에 작성]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**\n",
    "* 3. task1의 데이터를 활용하여 자유롭게 결과를 출력하고, 왜 그러한 결과를 출력했는지와 코드에 대한 설명을 추가하세요.(10 point)\n",
    "\n",
    "    **<반드시 포함되어야할 사항>**\n",
    "    \n",
    "    1. pyspark 함수가 아닌 **SQL 명령어 사용**. 또한, ``GROUP BY``, ``HAVING`` 반드시 포함되어야 함\n",
    "    2. 결과에 대한 설명(print로 작성하시오)\n",
    "    3. 주석 반드시 포함(코드에 대한 주석 #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3 답안 작성\n",
    "# 여러 셀 사용 가능\n",
    "# 결과에 대한 설명은 print() 활용 ---> print(\"[결과에 대한 설명] \\n\")\n",
    "# 각 코드에 대한 주석 첨부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수고ㅋ!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
