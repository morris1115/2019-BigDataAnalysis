{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiC6cm3phVRZ"
   },
   "source": [
    "# DataFrame\n",
    "\n",
    " - - -\n",
    "\n",
    "이번 notebook에서는 Spark RDD의 sub-type인 DataFrame에 대해 학습을 진행합니다.\n",
    "\n",
    "* Dataframes are a restricted sub-type of RDDs. \n",
    "* Restircing the type allows for more optimization.\n",
    "* Dataframes store two dimensional data, similar to the type of data stored in a spreadsheet(ex. MS Excel). \n",
    "   * Each column in a dataframe can have a different type.\n",
    "   * Each row contains a `record`.\n",
    "   \n",
    "**Spark의 DataFrame은 python pandas의 DataFrame, R의 DataFrame과 매우 유사합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41Wx6xo9hVRc"
   },
   "source": [
    "### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rC2rKUjEhVRe",
    "outputId": "ec2a938b-f2b2-4217-c739-30877fcf6653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f043c57d9e8>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gGmaxO0hVRj"
   },
   "source": [
    "### Constructing a DataFrame from an RDD of Rows\n",
    "Each Row defines it's own  fields, the schema is *inferred*.\n",
    "\n",
    "여기서, ``schema is inferred``는 Spark가 스스로 각 column의 데이터의 유형을 파악하고, 그에 맞는 형식을 제공한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keAPXgakhVRk",
    "outputId": "3a2c8ef8-dc62-4a52-f67b-83f4b3fba921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name='John'),\n",
       " Row(age=23, name='Smith'),\n",
       " Row(age=18, name='Sarah')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to create a DataFrame is to first define an RDD from a list of Rows \n",
    "some_rdd = sc.parallelize([Row(name=\"John\", age=19),\n",
    "                           Row(name=\"Smith\", age=23),\n",
    "                           Row(name=\"Sarah\", age=18)])\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brpscenqhVRo",
    "outputId": "115cb8dd-c738-4ad9-96db-0dffc01b37b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame is created from the RDD or Rows\n",
    "# Infer schema from the first row, create a DataFrame and print the schema\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "some_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DTqPpUShVRr",
    "outputId": "4c436f6b-cd8c-48ce-e974-265ea9ded962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RDD(some_rdd) =  <class 'pyspark.rdd.RDD'> \n",
      " DataFrame(some_df) =  <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "some_df = [Row(age=19, name='John'), Row(age=23, name='Smith'), Row(age=18, name='Sarah')]\n",
      "some_rdd= [Row(age=19, name='John'), Row(age=23, name='Smith'), Row(age=18, name='Sarah')]\n"
     ]
    }
   ],
   "source": [
    "# A dataframe is an RDD of rows plus information on the schema.\n",
    "# performing **collect()* on either the RDD or the DataFrame gives the same result.\n",
    "print(\" RDD(some_rdd) = \", type(some_rdd),\"\\n\",\"DataFrame(some_df) = \", type(some_df), \"\\n\")\n",
    "print('some_df =',some_df.collect())\n",
    "print('some_rdd=',some_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifsfYhJKhVRu"
   },
   "source": [
    "### Defining the Schema explicitly\n",
    "The advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows.\n",
    "\n",
    "Spark가 ``schema``를 추론(inferred)하는 것이 아닌, 사용자가 직접 ``schema를 정의``할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPyou4_whVRv",
    "outputId": "abfc1558-b76c-4938-d878-658a4d1f2e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_name: string (nullable = false)\n",
      " |-- person_age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\n",
    "another_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n",
    "# Schema with two fields - person_name and person_age\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "\n",
    "# Create a DataFrame by applying the schema to the RDD and print the schema\n",
    "another_df = sqlContext.createDataFrame(another_rdd, schema)\n",
    "another_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObbJkJodhVRy"
   },
   "source": [
    "## Loading DataFrames from disk\n",
    "There are many methods to load DataFrames from Disk. Here we will discuss three of these methods\n",
    "1. JSON (on your own)\n",
    "2. CSV  (on your own)\n",
    "3. **Parquet**\n",
    "\n",
    "In addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNowuhSjhVRz"
   },
   "source": [
    "### Loading dataframes from JSON files\n",
    "[JSON](http://www.json.org/)은 ``속성-값 pair`` 또는 ``키-값 pair``으로 이루어진 데이터 오브젝트를 전달하기 위해 인간이 읽을 수 있는 텍스트를 사용하는 개방형 표준 포맷이다. 특히, 인터넷에서 자료를 주고 받을 때 그 자료를 표현하는 방법으로 알려져 있다. 자료의 종류에 큰 제한은 없으며, 특히 컴퓨터 프로그램의 변수값을 표현하는 데 적합하다. **JSON can also be used to store tabular data and can be easily loaded into a dataframe.**\n",
    "\n",
    "**( .json 예시 )**\n",
    "\n",
    "![json예시](https://wallees.files.wordpress.com/2018/04/593aa-screen2bshot2b2018-04-172bat2b4-56-002bpm.png?w=400&h=186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jvk3aUoAhVR0",
    "outputId": "034dd034-3025-4c6e-d970-00cece90eea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people is a <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1TZyM7Gfc6XWLot-L36TDV-JwySgHxGv4\", \"people.json\")\n",
    "data_file = \"./people.json\"\n",
    "\n",
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(data_file)\n",
    "print('people is a',type(people))\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "people.show()\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYyyfeCFhVR3"
   },
   "source": [
    "### Excercise 1 : Loading ``csv`` files into dataframes (30 point)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "아래에 제시되는 두 가지 방법을 이용하여 ``csv``을 dataframe으로 변환하고(schema infered), ``schema``를 직접 설정하여 dataframe으로 변환합니다(**두 가지 방법 중 선택**)\n",
    "\n",
    "* 1. pandas의 ``read_csv``를 이용하여 ``csv``파일을 불러온 뒤, SQLContext의 ``createDataFrame``을 이용하여 dataframe으로 변환합니다.(10 point)\n",
    "\n",
    "* 2. SQLContext의 ``read.csv``를 이용하여 ``csv``를 dataframe으로 변환합니다. (10 point)\n",
    "\n",
    "* 3. ``schema``를 직접 설정하여 dataframe으로 변환(task1 또는 task2의 방법을 이용) (10 point)\n",
    "\n",
    "**위의 방법 외에도 ``csv``를 불러오는 방법은 많습니다. 하지만, Excercise 1 에서는 task 1, task 2 방법으로 한정합니다.**\n",
    "\n",
    "**!!csv to dataframe 방법 [참고 주소](https://stackoverflow.com/questions/29936156/get-csv-to-spark-dataframe)**\n",
    "\n",
    "- - -\n",
    "**출력 예시**\n",
    "\n",
    "* show()와 printSchema()를 이용하여 결과를 출력합니다(task 1, task 2 모두)\n",
    "\n",
    "* ex) \n",
    "\n",
    "```\n",
    "# task 1 == pandas -> createDataFrame\n",
    "\n",
    "task1.show(3)\n",
    "task1.printSchema()\n",
    "\n",
    "# task 2 == SQLContext -> DataFrame\n",
    "\n",
    "task2.show(3)\n",
    "task2.printSchema()\n",
    "\n",
    "# task 3\n",
    "task3.show(3)\n",
    "task3.printSchema()\n",
    "\n",
    "### task3.printSchema() 예시\n",
    "root\n",
    " |-- batter_id: integer (nullable = true)\n",
    " |-- batter_name: string (nullable = true)\n",
    " |-- date: string (nullable = true)\n",
    " |-- opposing_team: string (nullable = true)\n",
    " |-- avg: double (nullable = true)\n",
    " |-- AB: integer (nullable = true)\n",
    " |-- R: integer (nullable = true)\n",
    " |-- H: integer (nullable = true)\n",
    " |-- 2B: integer (nullable = true)\n",
    " |-- 3B: integer (nullable = true)\n",
    " |-- HR: integer (nullable = true)\n",
    " |-- RBI: integer (nullable = true)\n",
    " |-- CS: integer (nullable = true)\n",
    " |-- BB: integer (nullable = true)\n",
    " |-- HBP: integer (nullable = true)\n",
    " |-- SO: integer (nullable = true)\n",
    " |-- GDP: integer (nullable = true)\n",
    " |-- avg2: double (nullable = true)\n",
    " |-- year: string (nullable = true)\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjPpCodThVR3",
    "outputId": "9713b042-f40f-4496-8e78-a1f65ae58e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\t\t\t\t  Lab_HW2_upload.ipynb\r\n",
      "ex2_coalesce\t\t\t  people.json\r\n",
      "ex2_raw_partition\t\t  pre.csv\r\n",
      "ex2_repartition\t\t\t  regular.csv\r\n",
      "HW1_20155138_안춘모.ipynb\t  Regular_Season_Batter_Day_by_Day.csv\r\n",
      "HW3_upload_V1.ipynb\t\t  save\r\n",
      "HW5_20155138_안춘모.ipynb\t  users.parquet\r\n",
      "kddcup.data_10_percent_corrected\r\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1QHSuh61Ng8JQ7JkAQfDZgLzd5auC9KAP\", \"Regular_Season_Batter_Day_by_Day.csv\")\n",
    "data_file = \"./Regular_Season_Batter_Day_by_Day.csv\"\n",
    "    \n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tleaZbW8N-cp"
   },
   "source": [
    "**task**\n",
    "* 1. pandas의 ``read_csv``를 이용하여 ``csv``파일을 불러온 뒤, SQLContext의 ``createDataFrame``을 이용하여 dataframe으로 변환합니다.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDVTs9GthVR7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|batter_id|batter_name|date|opposing_team| avg1| AB|  R|  H| 2B| 3B| HR|RBI| SB| CS| BB|HBP| SO|GDP|               avg2|year|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|        0|   가르시아|3.24|           NC|0.333|  3|  1|  1|  0|  0|  0|  0|  0|  0|  1|  0|  1|  0|              0.333|2018|\n",
      "|        0|   가르시아|3.25|           NC|0.000|  4|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|0.14300000000000002|2018|\n",
      "|        0|   가르시아|3.27|         넥센|0.200|  5|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|0.16699999999999998|2018|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- batter_id: long (nullable = true)\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      " |-- opposing_team: string (nullable = true)\n",
      " |-- avg1: string (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: long (nullable = true)\n",
      " |-- SB: long (nullable = true)\n",
      " |-- CS: long (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- HBP: long (nullable = true)\n",
      " |-- SO: long (nullable = true)\n",
      " |-- GDP: long (nullable = true)\n",
      " |-- avg2: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-1 답안 작성\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.read_csv('Regular_Season_Batter_Day_by_Day.csv')\n",
    "task1_DF = sqlContext.createDataFrame(pandas_df) \n",
    "# csv파일을 읽어와 pandas의 dataframe과 스키마를 출력한다.\n",
    "\n",
    "# output\n",
    "task1_DF.show(3)\n",
    "task1_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcVK1L0eN-cs"
   },
   "source": [
    "**task**\n",
    "* 2. SQLContext의 ``read.csv``를 이용하여 ``csv``를 dataframe으로 변환합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeMOAxJChVSA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "|      _c0|        _c1| _c2|          _c3|  _c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|               _c18|_c19|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "|batter_id|batter_name|date|opposing_team| avg1| AB|  R|  H| 2B| 3B|  HR| RBI|  SB|  CS|  BB| HBP|  SO| GDP|               avg2|year|\n",
      "|        0|   가르시아|3.24|           NC|0.333|  3|  1|  1|  0|  0|   0|   0|   0|   0|   1|   0|   1|   0|0.33299999999999996|2018|\n",
      "|        0|   가르시아|3.25|           NC|0.000|  4|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   1|   0|0.14300000000000002|2018|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-2 답안 작성\n",
    "task2_DF = sqlContext.read.csv('Regular_Season_Batter_Day_by_Day.csv') #csv파일을 읽어 csv파일을 스키마와 dataframe형태로 출력\n",
    "# output\n",
    "task2_DF.show(3)\n",
    "task2_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_rXKn89N-cw"
   },
   "source": [
    "**task**\n",
    "* 3. ``schema``를 직접 설정하여 dataframe으로 변환(task1 또는 task2의 방법을 이용) (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OBy8RvihVSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- better_id: long (nullable = true)\n",
      " |-- better_name: string (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      " |-- opposing_team: string (nullable = true)\n",
      " |-- avg1: string (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: long (nullable = true)\n",
      " |-- SB: long (nullable = true)\n",
      " |-- CS: long (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- HBP: long (nullable = true)\n",
      " |-- SO: long (nullable = true)\n",
      " |-- GDP: long (nullable = true)\n",
      " |-- avg2: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+---------+-----------+----+-------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "|better_id|better_name|date|opposing_team| avg1|  AB|   R|   H|  2B|  3B|  HR| RBI|  SB|  CS|  BB| HBP|  SO| GDP|               avg2|year|\n",
      "+---------+-----------+----+-------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "|     null|       null|null|         null| null|null|null|null|null|null|null|null|null|null|null|null|null|null|               null|null|\n",
      "|        0|   가르시아|3.24|           NC|0.333|   3|   1|   1|   0|   0|   0|   0|   0|   0|   1|   0|   1|   0|0.33299999999999996|2018|\n",
      "|        0|   가르시아|3.25|           NC|0.000|   4|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|0.14300000000000002|2018|\n",
      "+---------+-----------+----+-------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-3 답안 작성\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, DoubleType, LongType\n",
    "task3_schema = StructType([StructField(\"better_id\", LongType(), True),\n",
    "                          StructField(\"better_name\", StringType(),True),\n",
    "                          StructField(\"date\", DoubleType(), True),\n",
    "                          StructField(\"opposing_team\",StringType(),True),\n",
    "                          StructField(\"avg1\",StringType(),True),\n",
    "                          StructField(\"AB\",LongType(),True),\n",
    "                          StructField(\"R\",LongType(),True),\n",
    "                          StructField(\"H\",LongType(),True),\n",
    "                          StructField(\"2B\",LongType(),True),\n",
    "                          StructField(\"3B\",LongType(),True),\n",
    "                          StructField(\"HR\",LongType(),True),\n",
    "                          StructField(\"RBI\",LongType(),True),\n",
    "                          StructField(\"SB\",LongType(),True),\n",
    "                          StructField(\"CS\",LongType(),True),\n",
    "                          StructField(\"BB\",LongType(),True),\n",
    "                          StructField(\"HBP\",LongType(),True),\n",
    "                          StructField(\"SO\",LongType(),True),\n",
    "                          StructField(\"GDP\",LongType(),True),\n",
    "                          StructField(\"avg2\",DoubleType(),True),\n",
    "                          StructField(\"year\",LongType(),True),])\n",
    "# 스키마를 생성하는데 타입을 맞춰서 선언 하고, null값을 의미하는 세번째 인자를 허용한다는 뜻에서 True로 설정한다.\n",
    "\n",
    "# output\n",
    "task3_DF = sqlContext.read.csv(data_file,task3_schema) # data_file에서 csv파일을 읽어와 새로만든 스키마에 적용한다.\n",
    "task3_DF.printSchema()\n",
    "task3_DF.show(3) # dataframe형태로 3줄 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nh-gfNqNhVSH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CTk93_hhVSN"
   },
   "source": [
    "### Loading dataframes from Parquet\n",
    "[Parquet](https://en.wikipedia.org/wiki/Apache_Parquet)은 ``중첩된 데이터를 효율적으로 저장할 수 있는 컬럼 기준 저장 포맷``\n",
    "\n",
    "**자세한 설명은 아래의 주소를 참고하세요**\n",
    "\n",
    "* http://engineering.vcnc.co.kr/2018/05/parquet-and-spark/\n",
    "\n",
    "* https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8\n",
    "\n",
    "* http://parquet.apache.org/\n",
    "\n",
    "\n",
    "\n",
    "**( Parquet 예시 )**\n",
    "\n",
    "![json예시](http://engineering.vcnc.co.kr/images/2018/05/parquet-data-io-philadelphia-2013-8-638.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clMbY8sdhVSO",
    "outputId": "a2014e18-a3ec-45cb-9273-49cab52f64c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1FKoN6JB34LIvYF571xHrLyVFduj5n-Kj\", \"users.parquet\")\n",
    "data_file = \"./users.parquet\"\n",
    "\n",
    "df = sqlContext.read.load(data_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5pDdyr8hVSR"
   },
   "source": [
    "## Lets have a look at a real-world dataframe\n",
    "\n",
    "This dataframe is a small part from a large dataframe (15GB) which stores meteorological data from stations around the world. We will read the dataframe from a zipped parquet file.\n",
    "\n",
    "- - -\n",
    "\n",
    "``Spark가 parquet format을 load 하는 메소드는 1줄``\n",
    "\n",
    "``parquet가 어떤 형태로 되어있는지 반드시 확인할 것!``\n",
    "\n",
    "```\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7d6Dlv0QhVSS",
    "outputId": "61f53ed8-1223-4c4c-cbf5-2c8210755e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work <class 'str'>\n",
      "/home/jovyan/work/Data\n",
      "/home/jovyan/work/Data/Weather\n"
     ]
    }
   ],
   "source": [
    "from os.path import split,join,exists\n",
    "from os import mkdir,getcwd,remove\n",
    "from glob import glob\n",
    "\n",
    "# create directory if needed\n",
    "\n",
    "notebook_dir=getcwd()\n",
    "print(notebook_dir, type(notebook_dir))\n",
    "data_dir=join(notebook_dir,'Data')\n",
    "weather_dir=join(data_dir,'Weather')\n",
    "print(data_dir)\n",
    "print(weather_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ib9aGNmhVSV",
    "outputId": "b866f245-1e1a-464d-8745-824e13002ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory /home/jovyan/work/Data already exists\n",
      "directory /home/jovyan/work/Data/Weather already exists\n",
      "NY.\n",
      "['/home/jovyan/work/Data/Weather/NY.parquet', '/home/jovyan/work/Data/Weather/NY.tgz']\n",
      "removing /home/jovyan/work/Data/Weather/NY.parquet\n",
      "removing /home/jovyan/work/Data/Weather/NY.tgz\n"
     ]
    }
   ],
   "source": [
    "# Initializing the directory\n",
    "if exists(data_dir):\n",
    "    print('directory',data_dir,'already exists')\n",
    "else:\n",
    "    print('making',data_dir)\n",
    "    mkdir(data_dir)\n",
    "\n",
    "if exists(weather_dir):\n",
    "    print('directory',weather_dir,'already exists')\n",
    "else:\n",
    "    print('making',weather_dir)\n",
    "    mkdir(weather_dir)\n",
    "\n",
    "file_index='NY'\n",
    "zip_file='%s.tgz'%(file_index)\n",
    "print(zip_file[:-3])\n",
    "\n",
    "# For linux\n",
    "old_files='%s/%s*'%(weather_dir,zip_file[:-3])\n",
    "print(glob(old_files))\n",
    "for f in glob(old_files):\n",
    "    print('removing',f)\n",
    "#   For Linux\n",
    "    !rm -rf {f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10R1aXKyhVSZ",
    "outputId": "5dbb1000-2726-4246-cd08-5f052166461f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYigK-ZuhVSc",
    "outputId": "83026e35-fbee-4ad7-f88a-f163a3d54f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1hAHV6vC6FvVgrYnoN-lR-IfH488-H121 into Data/Weather/NY.tgz... Done.\n",
      "-rwxr-xr-x 1 root root 64M Oct 28 16:31 /home/jovyan/work/Data/Weather/NY.tgz\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = 'Data/Weather/NY.tgz')\n",
    "\n",
    "!ls -lh $weather_dir/$zip_file\n",
    "\n",
    "#extracting the parquet file\n",
    "#!tar zxvf {weather_dir}/{zip_file} -C {weather_dir}\n",
    "\n",
    "\n",
    "tf = tarfile.open(join(weather_dir,zip_file), mode=\"r\")\n",
    "tf.extractall(weather_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEwpbU3ChVSf",
    "outputId": "46480240-ddb6-4f42-bc98-1366f06749a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Data/Weather/NY.parquet\n",
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_parquet = join(weather_dir, zip_file[:-3]+'parquet')\n",
    "print(weather_parquet)\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oreWDWNwhVSi"
   },
   "source": [
    "## Writing DataFrames to CSV, JSON, PARQUET\n",
    "\n",
    "생성된 DataFrame을 ``.write.[csv/json/parquet](\"폴더이름\")`` 를 통해서 write 가능\n",
    "\n",
    "\n",
    "\n",
    "설정된 ``폴더이름``에 ``DataFrame이 파티션(분할)되어 지정한 format으로 저장됨``.\n",
    "\n",
    "```\n",
    "df.write.csv(\"폴더이름\")\n",
    "df.write.json(\"폴더이름\")\n",
    "df.write.parquet(\"폴더이름\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXVp_I6MhVSp",
    "outputId": "799a1b2c-45fd-44a9-d4c6-50e30dbb7376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save directory already exists\n",
      "csv_test  json_test  parquet_test\n"
     ]
    }
   ],
   "source": [
    "if exists(\"save\"):\n",
    "    print('save directory already exists')\n",
    "else:\n",
    "    mkdir(\"save\")\n",
    "    df.write.csv(\"save/csv_test\")\n",
    "    df.write.json(\"save/json_test\")\n",
    "    df.write.parquet(\"save/parquet_test\")\n",
    "\n",
    "!ls save # save 폴더 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuY1RCevhVSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnTfdvOwhVSu"
   },
   "source": [
    "### Excercise 2 : ``coalesce `` or ``repartition``!! (30 point)\n",
    "\n",
    "Spark는 기본적으로 RDD or DataFrame or 등등.. 을 생성할 때, Spark Core 개수만큼 파티션을 설정한다. 예를 들어 어떠한 RDD를 생성할 때, Spark Core가 7개라면 7개의 파티션으로 처리한다. 단, DataFrame의 size에 따라 자동변경 될 수도 있다.\n",
    "\n",
    "``coalesce``와 ``repartition``는 무엇인가....?\n",
    "\n",
    "``coalesce``와 ``repartition`` 파티션 개수를 줄이거나 늘리는 데 사용한다. \n",
    "```\n",
    "#coalesce \n",
    "coalesce (numPartitions: Int, shuffle: Boolean = false)\n",
    "```\n",
    "\n",
    "자세한 설명은 [여기1](https://thebook.io/006908/part01/ch04/02/03/02/), [여기2](https://knight76.tistory.com/entry/scala-spark%EC%97%90%EC%84%9C-partition-%EC%A4%84%EC%9D%B4%EA%B8%B0-repartition-coalesce), [여기3](https://m.blog.naver.com/PostView.nhn?blogId=8x8x8x8x8x8&logNo=220740234992&proxyReferer=https%3A%2F%2Fwww.google.com%2F)을 클릭하세요.\n",
    "\n",
    "또한, Spark의 Shuffling에 대해 이해를 해야 합니다. [여기4](https://swalloow.github.io/spark-shuffling)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "``coalesce``와 ``repartition``을 이용하여 Exercise 1에서 생성된 DataFrame의 파티션을 수정하여 ``parquet``로 저장합니다.\n",
    "\n",
    "* 1. [Dataframe].rdd.getNumPartitions() 으로 partition 값을 확인합니다.\n",
    "\n",
    "* 2. **partition을 수정하지 않고** DataFrame을 parquet로 저장후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (5 point)\n",
    "\n",
    "* 3. ``repartition``을 이용하여 DataFrame의 **partition을 2에서 10으로 증가**시킵니다. **partition이 증가된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "* * 4. coalesce를 이용하여 partition의 수를 1로 감소 시킵니다. **partition이 감소된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 pt)\n",
    "\n",
    "* 5. 첨부된 링크를 이용하여 ``coalesce``와 ``repartition``의 **차이점을 2 ~ 3 문장으로 서술하세요.** (5 point)\n",
    "- - -\n",
    "**출력 예시(세부 값(주소)은 다를 수 있음)**\n",
    "```\n",
    "# task 4\n",
    "part-00000-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00001-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00002-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00003-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00004-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGdoOmlsN-dT"
   },
   "source": [
    "**task**\n",
    "- 1. [Dataframe].rdd.getNumPartitions() 으로 partition 값을 확인합니다.\n",
    "\n",
    "``#output``\n",
    "```\n",
    "Ex1_df partition number :  4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGM18iLiN-dU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex1_df partition number :  1\n"
     ]
    }
   ],
   "source": [
    "# task 2 -1 답안 작성\n",
    "Ex1_df = sqlContext.read.load(data_file) # data_file의 파티션의 개수를 출력한다.\n",
    "\n",
    "# output\n",
    "print(\"Ex1_df partition number : \", Ex1_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l40GPEIeN-dX"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 2. **partition을 수정하지 않고** DataFrame을 parquet로 저장후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (5 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00001-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00002-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00003-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDF6m9i7hVSv",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-fe33c956-b9d5-4445-a1e8-e5718e47d669-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# task 2-2 답안 작성\n",
    "Ex1_df.write.parquet(\"ex2_raw_partition\") #ex2_raw_partition을 parquet파일로 저장한다.\n",
    "\n",
    "# output\n",
    "!ls ex2_raw_partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0GGgMJWN-da"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 3. ``repartition``을 이용하여 DataFrame의 **partition을 4에서 10으로 증가**시킵니다. **partition이 증가된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00001-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00002-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00003-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00004-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00005-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00006-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00007-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00008-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00009-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R_iqwxehVSy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-c4614c07-eab4-4f59-b1bd-2f03e84063ad-c000.snappy.parquet\r\n",
      "part-00001-c4614c07-eab4-4f59-b1bd-2f03e84063ad-c000.snappy.parquet\r\n",
      "part-00002-c4614c07-eab4-4f59-b1bd-2f03e84063ad-c000.snappy.parquet\r\n",
      "_SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# task 2-3 답안 작성\n",
    "Ex1_df.repartition(10).write.parquet(\"ex2_repartition\") \n",
    "# \"ex2_repartition을 repartition(10)을 이용해 파티션의 개수를 증가시킨 후 parquet형태로 저장한다.\"\n",
    "\n",
    "# output\n",
    "!ls ex2_repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKPH87gBN-dd"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 4. coalesce를 이용하여 partition의 수를 1로 감소 시킵니다. **partition이 감소된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-01b5e415-ddcc-4d69-9478-4e2db153f103-c000.snappy.parquet  _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmwwGkVohVS1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-38931ca9-3047-42dc-a5f8-728ab7065ef4-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# task 2-4 답안 작성\n",
    "Ex1_df.coalesce(1).write.parquet(\"ex2_coalesce\") \n",
    "#10개로 늘렸던(실제로는 컴퓨터의 사양에 의해 3개 밖에 출력 안됨)파티션의 수를 coalsece를 사용해 파티션의 개수를 감소시킨 후 \n",
    "#write를 사용해 parquet형태로 저장한다.\n",
    "\n",
    "# output\n",
    "!ls ex2_coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZ53bai3hVS4"
   },
   "source": [
    "### task 2-5 답안작성 (5 point)\n",
    "#### coalsece는 partition의 개수를 감소시키는 역할을 하고, repartitiond은 partition의 개수를 증가시키는데 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9BTkTv6hVS5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGpRsMC8hVS7"
   },
   "source": [
    "## DataFrame Operations\n",
    "```\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "df2 = sqlContext\\\n",
    ".read.csv(\"Regular_Season_Batter_Day_by_Day.csv\", header=True)\n",
    "\n",
    "```\n",
    "\n",
    "위에서 사용했던 DataFrame을 다시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7Eriko4hVS8",
    "outputId": "9048b632-ef33-451a-a3c2-ad87bec0353c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load(weather_parquet)\n",
    "df2 = sqlContext\\\n",
    ".read.csv(\"Regular_Season_Batter_Day_by_Day.csv\", header=True)\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qsZ0fkFhVS-"
   },
   "source": [
    "### 1. columns & dtypes\n",
    "\n",
    "``columns`` : DataFrame의 column명 반환\n",
    "\n",
    "``dtypes``  : DataFrame의 column명 및 데이터 유형 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXNdW799hVS_",
    "outputId": "cd1fe476-63cb-4e45-ac23-edf756b9f716",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station', 'Measurement', 'Year', 'Values', 'dist_coast', 'latitude', 'longitude', 'elevation', 'state', 'name'] \n",
      "\n",
      "[('Station', 'string'), ('Measurement', 'string'), ('Year', 'bigint'), ('Values', 'binary'), ('dist_coast', 'double'), ('latitude', 'double'), ('longitude', 'double'), ('elevation', 'double'), ('state', 'string'), ('name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(df.columns, \"\\n\")   # columns를 이용한 DataFrame의 column 명 추출\n",
    "print(df.dtypes)          # dtypes를 이용한 column 명 및 type 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NM8xq_7hVTC"
   },
   "source": [
    "### 2. show, select, drop, filter\n",
    "\n",
    "``show(n = 20, truncate = True or False)`` : 처음 n행 출력. truncate는 정렬 여부\n",
    "\n",
    "``select(col)``  : DataFrame의 지정된 column 반환\n",
    "\n",
    "``drop(col)``    : DataFrame의 지정된 column 삭제\n",
    "\n",
    "``filter(condition)`` : True or False로 평가되는 condition 인수에 의한 DataFrame 반환\n",
    "\n",
    "``distinct()`` : 모든 column의 모든 값이 동일한 row 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2IQOUayhVTD",
    "outputId": "225e0466-e91b-4f95-863c-9ceb0cfee9d8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1950|[6E 4B 93 4B BB 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1951|[27 4A 32 4A 28 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1952|[54 4B 60 4B 6A 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1953|[48 4A 37 4A 28 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|2000|[DE 4A D4 4A CA 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "df.show(10) # n = 10, truncate=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqoa8x-mhVTF",
    "outputId": "7f46bb35-17b4-429a-eb04-274133a3a516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----+\n",
      "|    Station|Measurement|Year|              Values|state|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|   NY|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|   NY|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select(col)\n",
    "df.select(\"Station\", \"Measurement\", \"Year\", \"Values\", \"state\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRLfa4pBhVTH",
    "outputId": "3da287d8-f28b-4e4f-e8e0-84e1ceb1302e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop(col)\n",
    "df.drop(\"state\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiCtPGHthVTX"
   },
   "source": [
    "### 4. Aggregations\n",
    "\n",
    "* **Aggregation** can be used, in combination with built-in sparkSQL functions \n",
    "to compute statistics of a dataframe.\n",
    "\n",
    "* computation will be fast thanks to combined optimzations with database operations.\n",
    "\n",
    "* A partial list : `count(), approx_count_distinct(), avg(), max(), min()`\n",
    "\n",
    "* Of these, the interesting one is `approx_count_distinct()` which uses sampling to get an approximate count fast. (`approximation`(근사치)를 이용해 빠른 결과값 도출 가능)\n",
    "\n",
    "* pyspark에서 이용 가능한 모든 Aggregations function은 [여기서 확인 하시오!!!](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import [사용하고자 하는 function]\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "아래의 예제부터는 DataFrame에서 특정 column을 선택할 때, col() 사용하여 진행하겠습니다.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iheHpmEoN-d0",
    "outputId": "5be82d2e-298b-42ce-9171-8abe39d55f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load(weather_parquet)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJuFBIwfN-d3"
   },
   "source": [
    "#### (1) Count, CountDistinct, approx_count_distinct\n",
    "\n",
    "**전체 row의 수 count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DG32qSUcN-d4",
    "outputId": "0ebf30ff-a5d7-49af-bc71-620a383689cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, approx_count_distinct\n",
    "\n",
    "df.select(count(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aqe4LzTIN-eA"
   },
   "source": [
    "``countDistinct``는 고유한(중복없이) row의 수를 반환합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OKzZtWrN-eB",
    "outputId": "b352a75e-c139-4ff2-cd47-01d429ac0d63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Station)|\n",
      "+-----------------------+\n",
      "|                    343|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpe6nZIpN-eD"
   },
   "source": [
    "approx_count_distinct는 row의 근사치를 반환합니다.\n",
    "\n",
    "* approx_count_distinct는 최대 추정 오류율(maximum estimation error)의 값을 인자로 받는데, 자신이 오류를 어디까지 허용하는지에 따라 값을 주면됩니다.\n",
    "\n",
    "* count_distinct를 통해 얻는 실제값과의 차이는 최대 추정 오류의 수치에 따라 상이합니다.\n",
    "\n",
    "* **하지만, 연산은 count_distinct보다 더 빠르게 결과를 반환합니다. 즉, 대규모 데이터셋을 파악할 때 사용될 수 있습니다.!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUsL2QqfN-eE",
    "outputId": "a76dc001-ff9f-4000-fce0-a06c02211606",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|approx_count_distinct(Station)|\n",
      "+------------------------------+\n",
      "|                           330|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(col(\"Station\"), 0.2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAoYP1LNN-eH"
   },
   "source": [
    "#### (2) first, last\n",
    "첫 번째 row와 마지막 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4GupSuuN-eI",
    "outputId": "15a5832f-4110-4b2e-af07-f46db9478be0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(col(\"Station\")), last(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQv0JsH3N-eM"
   },
   "source": [
    "#### (3) min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XNKLHJkN-eN",
    "outputId": "67f04bb6-450d-460c-c582-488b9b6ccba6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     1871|     2013|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(col(\"year\")), max(col(\"year\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4CgeJLMkN-eT"
   },
   "source": [
    "#### (4) sum, sumDistinct\n",
    "\n",
    "``sumDistinct``는 고유값(중복 없는)의 sum!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcWXVxyrN-eU",
    "outputId": "137a96f7-4ee8-43bb-9de5-1fcb20a456e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    sum(dist_coast)|\n",
      "+-------------------+\n",
      "|4.138962684120101E7|\n",
      "+-------------------+\n",
      "\n",
      "+------------------------+\n",
      "|sum(DISTINCT dist_coast)|\n",
      "+------------------------+\n",
      "|       76862.93162703142|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "\n",
    "df.select(sum(col(\"dist_coast\"))).show()\n",
    "\n",
    "df.select(sumDistinct(col(\"dist_coast\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2K8EVfEN-eW"
   },
   "source": [
    "#### (5) avg, alias\n",
    "\n",
    "``alias``는 SQL에서 ``as``와 동일한 기능을 합니다. 집계된 column을 재활용하기 위해 이름을 설정한다고 생각하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utwpxpKeN-eX",
    "outputId": "b18a7e10-6c7b-4b8f-f3f3-62c901dc7ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|         avg(year)|   avg(latitude)|\n",
      "+------------------+----------------+\n",
      "|1963.4289124573927|42.6842968505041|\n",
      "+------------------+----------------+\n",
      "\n",
      "+------------------+----------------+\n",
      "|          year_avg|    latitude_avg|\n",
      "+------------------+----------------+\n",
      "|1963.4289124573927|42.6842968505041|\n",
      "+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(col(\"year\")), avg(col(\"latitude\"))).show()\n",
    "\n",
    "# alias를 사용하게 되면...\n",
    "df.select(avg(col(\"year\")).alias(\"year_avg\"), \n",
    "          avg(col(\"latitude\")).alias(\"latitude_avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qq_MA5ChVTQ"
   },
   "source": [
    "#### (6) agg and groupby\n",
    "The method `.agg(spec)` computes a summary for each group as specified in `spec`\n",
    "The method `.groupby(col)` groups rows according the value of the column `col`.  \n",
    "\n",
    "``groupy``로 원하는 **column의 데이터를 그룹화**하고, ``agg``를 통해 여러 aggregation을 동시에 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEaVEz7cN-ea"
   },
   "source": [
    "``agg의 사용 에시``\n",
    "```\n",
    "[DataFrame].agg({'colum1':'[aggregation]', 'coulum2':'[aggregation]', ...})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADMkGuRFN-eb",
    "outputId": "b2c23700-d65f-4db9-c3f9-d3de3117eecd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------+\n",
      "|    min(dist_coast)|         avg(year)|    max(latitude)|\n",
      "+-------------------+------------------+-----------------+\n",
      "|0.04799420014023781|1963.4289124573927|44.93579864501953|\n",
      "+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'year':'avg', 'latitude':'max', 'dist_coast':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_c6uxM7ThVTR",
    "outputId": "d38766b5-01f2-4781-85ca-f4ece25dbd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+\n",
      "|Measurement|min(year)|count(state)|\n",
      "+-----------+---------+------------+\n",
      "|   TMIN_s20|     1873|       13442|\n",
      "|       TMIN|     1873|       13442|\n",
      "|   SNOW_s20|     1884|       15629|\n",
      "|       TOBS|     1876|       10956|\n",
      "|   SNWD_s20|     1888|       14617|\n",
      "|   PRCP_s20|     1871|       16118|\n",
      "|   TOBS_s20|     1876|       10956|\n",
      "|       TMAX|     1873|       13437|\n",
      "|       SNOW|     1884|       15629|\n",
      "|   TMAX_s20|     1873|       13437|\n",
      "|       SNWD|     1888|       14617|\n",
      "|       PRCP|     1871|       16118|\n",
      "+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(col('Measurement')).agg({'year': 'min', 'state':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQpm6H0yhVTS",
    "outputId": "9d17c325-46bc-4e03-e84c-1ce82ed953ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|    station|min(year)|\n",
      "+-----------+---------+\n",
      "|USC00303955|     1992|\n",
      "|USW00093732|     1958|\n",
      "|USW00014786|     1945|\n",
      "|USC00300621|     1950|\n",
      "|USC00301387|     1926|\n",
      "|USC00305426|     1896|\n",
      "|USC00306659|     1898|\n",
      "|USC00303124|     1971|\n",
      "|USC00303983|     1950|\n",
      "|USC00300343|     1895|\n",
      "|USC00305441|     1973|\n",
      "|USC00303050|     1948|\n",
      "|USC00300360|     1907|\n",
      "|USW00004742|     1956|\n",
      "|USC00301401|     1902|\n",
      "|USC00306817|     1893|\n",
      "|USC00308104|     1901|\n",
      "|USC00305769|     1985|\n",
      "|USC00303889|     1926|\n",
      "|USC00306019|     1942|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(col('station')).agg({'year': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeGMzwjfhVTV",
    "outputId": "d8346c81-5de6-4db6-b125-9fb92130ec50",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------------+\n",
      "|state|   avg(latitude)|   avg(longitude)|\n",
      "+-----+----------------+-----------------+\n",
      "|   NY|42.6842968505041|-75.4551864389521|\n",
      "+-----+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('state').agg({'latitude':'mean', 'longitude':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0w1mntDhVTJ"
   },
   "source": [
    "#### (7) describe()\n",
    "\n",
    "The method `df.describe()` computes five statistics for each column of the dataframe `df`.\n",
    "\n",
    "    The statistics are: **count, mean, std, min,max**\n",
    "\n",
    "R의 summary와 같은 기능. 각 column의 통계치를 간단하게 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fI6mRDH6hVTJ",
    "outputId": "0e1da310-c3bd-4f9d-89c4-b03879828331",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "|summary|    Station|Measurement|              Year|         dist_coast|          latitude|         longitude|         elevation| state|           name|\n",
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "|  count|     168398|     168398|            168398|             168398|            168398|            168398|            168398|168398|         168398|\n",
      "|   mean|       null|       null|1963.4289124573927| 245.78455113006692|  42.6842968505041| -75.4551864389521| 245.2899639266881|  null|           null|\n",
      "| stddev|       null|       null|30.586766032145405| 129.97112783972682|1.0492530244970353|1.7907915903419898| 189.6934270109707|  null|           null|\n",
      "|    min|USC00300015|       PRCP|              1871|0.04799420014023781| 39.79999923706055|-79.58560180664062|-999.9000244140625|    NY|      ADAMS CTR|\n",
      "|    max|USW00094794|   TOBS_s20|              2013| 476.80999755859375| 44.93579864501953|-71.94999694824219| 838.2000122070312|    NY|YOUNGSTOWN 2 NE|\n",
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()    # 전체 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyok4C8bhVTO",
    "outputId": "08d1b696-e1cf-4283-ac7f-82107c78b339",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+\n",
      "|summary|    station|              year|\n",
      "+-------+-----------+------------------+\n",
      "|  count|     168398|            168398|\n",
      "|   mean|       null|1963.4289124573927|\n",
      "| stddev|       null|30.586766032145405|\n",
      "|    min|USC00300015|              1871|\n",
      "|    max|USW00094794|              2013|\n",
      "+-------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select(col('summary'),col('station'),\n",
    "                    col('year')).show() # select를 이용한 year column 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E2Qjx6ShVTZ"
   },
   "source": [
    "### 5. Join\n",
    "\n",
    "**왼쪽과 오른쪽**의 데이터셋에 있는 ``하나 이상의 key(키)값``을 비교하고 왼쪽과 오른쪽 데이터셋의 결합 여부를 결정하는 ``조인 표현식(join expression)``의 평가 결과에 따라 두 개의 데이터셋를 조인합니다.\n",
    "\n",
    "- - -\n",
    "\n",
    "**조인 타입**\n",
    "\n",
    "```\n",
    "- 내부 조인(inner join) : 왼쪽과 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 외부 조인(outer join) : 왼쪽이나 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 왼쪽 외부 조인(left outer join) : 왼쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 오른쪽 외부 조인(right outer join) : 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 왼쪽 세미 조인(left semi join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 키가 일치하는 왼쪽 데이터셋만 유지\n",
    "- 왼쪽 안티 조인(left anti join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터셋만 유지\n",
    "- 자연 조인(natural join) : 두 데이터셋에서 동일한 이름을 가진 column을 암시적(implicit)으로 결합하는 조인\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PaKrnZThVTZ"
   },
   "outputs": [],
   "source": [
    "#### 예제 수행을 위한 DataFrame 생성.... 실제로는 DataFrame을 만드는 경우는 거의 없다...!\n",
    "\n",
    "person_RDD = sc.parallelize([Row(ID=0, NAME=\"Bill Chambers\",GRADUATE_PROGRAM=0,\n",
    "                                 spark_status = [100]),\n",
    "                             Row(ID=1, NAME=\"Matei Zaharia\",GRADUATE_PROGRAM=1,\n",
    "                                 spark_status = [500, 250, 100]),\n",
    "                             Row(ID=2, NAME=\"Michael Armbrust\",GRADUATE_PROGRAM=1,\n",
    "                                 spark_status = [250, 100])\n",
    "                            ])\n",
    "graduateProgram_RDD = sc.parallelize([Row(ID=0, DEGREE=\"Masters\",\n",
    "                                      DEPARTMENT=\"School of Information\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                                  Row(ID=2, DEGREE=\"Masters\",\n",
    "                                      DEPARTMENT=\"EECS\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                                  Row(ID=1, DEGREE=\"PH.D\",\n",
    "                                      DEPARTMENT=\"EECS\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                            ])\n",
    "spark_status_RDD = sc.parallelize([Row(ID=500, status=\"Vice President\"),\n",
    "                                   Row(ID=250, status=\"PMC Member\"),\n",
    "                                   Row(ID=100, status=\"Contributor\")\n",
    "                                  ])\n",
    "\n",
    "\n",
    "person_DF = sqlContext.createDataFrame(person_RDD)\n",
    "graduateProgram_DF = sqlContext.createDataFrame(graduateProgram_RDD)\n",
    "sparkStatus_DF = sqlContext.createDataFrame(spark_status_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DCSWLRLhVTc",
    "outputId": "c7a32141-cab4-4aa8-b95a-6806cbf5005a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person's schema\n",
      "root\n",
      " |-- GRADUATE_PROGRAM: long (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- spark_status: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "graduateProgram's schema\n",
      "root\n",
      " |-- DEGREE: string (nullable = true)\n",
      " |-- DEPARTMENT: string (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- SCHOLL: string (nullable = true)\n",
      "\n",
      "sparkStatus's schema\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"person's schema\")\n",
    "person_DF.printSchema()\n",
    "print(\"graduateProgram's schema\")\n",
    "graduateProgram_DF.printSchema()\n",
    "print(\"sparkStatus's schema\")\n",
    "sparkStatus_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igWmoyb8hVTe"
   },
   "source": [
    "### (1) inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rnG2XjThVTh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# joinExpression 생성. 키값 설정!\n",
    "join_ex = person_DF[\"GRADUATE_PROGRAM\"] == graduateProgram_DF[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrqTOLRJhVTj",
    "outputId": "7edcfcb0-3a21-40ce-92c2-9d90d190ce5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|GRADUATE_PROGRAM| ID|            NAME|   spark_status| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_DF.join(graduateProgram_DF, join_ex).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6pwjvgGhVTm"
   },
   "source": [
    "### (2) outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aehrDpF6hVTo",
    "outputId": "d5d0929e-2942-4de5-9bd3-a2592e91fb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|GRADUATE_PROGRAM|  ID|            NAME|   spark_status| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|               0|   0|   Bill Chambers|          [100]|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|               1|   1|   Matei Zaharia|[500, 250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|               1|   2|Michael Armbrust|     [250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|            null|null|            null|           null|Masters|                EECS|  2|UC Berkeley|\n",
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_DF.join(graduateProgram_DF, join_ex, \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_YJW3TdhVTu"
   },
   "source": [
    "### (3) left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEzIdcovhVTv",
    "outputId": "ee57e603-a8b6-4f9d-c243-61f7874a3d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "| DEGREE|          DEPARTMENT| ID|     SCHOLL|GRADUATE_PROGRAM|  ID|            NAME|   spark_status|\n",
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "|Masters|School of Informa...|  0|UC Berkeley|               0|   0|   Bill Chambers|          [100]|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|               1|   1|   Matei Zaharia|[500, 250, 100]|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|               1|   2|Michael Armbrust|     [250, 100]|\n",
      "|Masters|                EECS|  2|UC Berkeley|            null|null|            null|           null|\n",
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUpqJZaIhVTy"
   },
   "source": [
    "### (4) left semi join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKgu6FrfhVTz",
    "outputId": "6a54b8b4-031d-4142-d218-218456d8387f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-----------+\n",
      "| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+-------+--------------------+---+-----------+\n",
      "|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|\n",
      "+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0rAt9z6hVT3"
   },
   "source": [
    "### (5) left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdtBzSR8hVT4",
    "outputId": "82f16d3b-e359-4f67-d270-471347e809a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+-----------+\n",
      "| DEGREE|DEPARTMENT| ID|     SCHOLL|\n",
      "+-------+----------+---+-----------+\n",
      "|Masters|      EECS|  2|UC Berkeley|\n",
      "+-------+----------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbE4zvfihVUF"
   },
   "source": [
    "## 6. ★★UDF(사용자 정의 함수)..★\n",
    "\n",
    "파이썬과 외부 라이브러리를 사용해서 사용자가 원하는 형태로 transformation 할 수 있다.\n",
    "\n",
    "**UDF는 하나 이상의 column을 입력으로 받고, 사용자 정의에 따라 return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTjjxXgghVUG",
    "outputId": "9f15f207-8326-4e63-e7db-6e998295af1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|   0|   0|\n",
      "|   1|   1|\n",
      "|   2|   4|\n",
      "|   3|   9|\n",
      "|   4|  16|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF = sqlContext\\\n",
    ".createDataFrame(sc.parallelize([Row(num1 = x, num2 = x*x) for x in range(5)]))\n",
    "udfExamDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTuXtIOXhVUN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "def power10(double_value):\n",
    "    return double_value ** 10\n",
    "\n",
    "power3udf = udf(power3)  # python 함수를 udf로 등록\n",
    "power10udf = udf(power10)  # python 함수를 udf로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C780u4w3hVUP",
    "outputId": "d994061d-1547-42f2-ae4d-5403dd361465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|power3(num1)|num2|\n",
      "+------------+----+\n",
      "|           0|   0|\n",
      "|           1|   1|\n",
      "|           8|   4|\n",
      "|          27|   9|\n",
      "|          64|  16|\n",
      "+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select(power3udf(col('num1')), col('num2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SB7mi38yhVUR",
    "outputId": "8eeb69ad-0c78-426a-feb4-534b8077addd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|power3(num1)|power10(num2)|\n",
      "+------------+-------------+\n",
      "|           0|            0|\n",
      "|           1|            1|\n",
      "|           8|      1048576|\n",
      "|          27|   3486784401|\n",
      "|          64|1099511627776|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select(power3udf(col('num1')),power10udf(col('num2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "narRT_AshVUT",
    "outputId": "d2b337e9-d216-4342-d19b-6bf53b9ca1f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+-------------+------------+-------------+\n",
      "|num1|num2|power3(num1)|power10(num1)|power3(num2)|power10(num2)|\n",
      "+----+----+------------+-------------+------------+-------------+\n",
      "|   0|   0|           0|            0|           0|            0|\n",
      "|   1|   1|           1|            1|           1|            1|\n",
      "|   2|   4|           8|         1024|          64|      1048576|\n",
      "|   3|   9|          27|        59049|         729|   3486784401|\n",
      "|   4|  16|          64|      1048576|        4096|1099511627776|\n",
      "+----+----+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select('num1', 'num2',power3udf('num1'), power10udf('num1'), \n",
    "                power3udf('num2'), power10udf('num2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVTdD76Dj0po"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D12DTy_1hVUp"
   },
   "source": [
    "## Exercise 3 -\n",
    "\n",
    "### DataFrame을 사용하여 HW4 Exercise 4 다시 풀기! (40 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "- pre.csv : KBO에서 활약한 타자들의 **역대 시범경기(정규시즌 직전에 여는 연습경기)** 성적\n",
    "\n",
    "**위의 두 데이터는 모두 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv와 pre.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "★[column type 변경 참고](https://stackoverflow.com/questions/52871560/how-to-typecast-spark-dataframe-columns-using-pyspark)★\n",
    "\n",
    "★[column name 변경 참고](https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.withcolumnrenamed?view=spark-dotnet)★\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "- 1. pandas 또는 .read.csv를 이용하여 ``regular.csv``와 ``pre.csv``를 각각의 DataFrame으로 만듭니다.\n",
    "\n",
    "\n",
    "- 2. 생성된 각각의 DataFrame에서 **타자 이름(batter_name), 타수(AB), 안타(H)을 ``.select(col)``를 이용하여 Transformation**합니다. **단, 새로운 DataFrame의 각 column type은 string, flot 또는 double로 변환합니다.** (10 point)\n",
    "\n",
    "\n",
    "- 3. task2에서 생성된 DataFrame 각각에 ``groupby``, ``agg``, ``udf`` 또는 ``임의의 함수를 자유롭게 적용``하여 **정규 시즌과 시범경기의 평균 타율을 구한 후 선수 이름(column 1), 평균 타율(column 2)로 구성된 DataFrame으로 Transformation 합니다(regular, pre 모두).** (10 point)\n",
    "\n",
    "\n",
    "- 4. task3의 각 DataFrame(regular, pre)를 **batter_name**을 기준으로 ``join``하여,**``역대 정규시즌 평균 타율이 역대 시범경기 평균 타율보다 높은``, 선수 이름(column 1)과 해당 선수의 역대 정규시즌 평균타율(column 2)로 구성된 DataFrame으로 Transformation 합니다.**(10 point)\n",
    "\n",
    "\n",
    "- 5. task4에서 생성된 DataFrame에 ``내림차순``을 적용하여 **상위 10명의 선수의 이름과 역대 정규시즌 평균타율 출력합니다. 단, column 명을 출력 예시와 같게 변경할 것.** (10 point)\n",
    "\n",
    "---\n",
    "\n",
    "**DataFrame join 할 때...★**\n",
    "\n",
    "    - DataFrame의 기준이 되는 column 명(키값)이 서로 같으면 오류가 발생합니다. \n",
    "\n",
    "    - 따라서 서로 다른 column 명을 사용해야 됩니다(alias 또는 withColumnRenamed를 사용하여 column 명 변경)\n",
    "\n",
    "    - JoinExpression을 잘 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5f-vwEN-ff"
   },
   "source": [
    "**task**\n",
    "- 1. pandas 또는 .read.csv를 이용하여 ``regular.csv``와 ``pre.csv``를 각각의 DataFrame으로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpBPA9-KhVUq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+-----+\n",
      "|batter_id|batter_name|year|team|  avg|  G| AB|  R|  H| 2B| 3B| HR| TB|RBI| SB| CS| BB|HBP| SO|GDP|  SLG|  OBP|  E|height/weight|       year_born|        position|                          career|starting_salary|  OPS|\n",
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+-----+\n",
      "|        0|   가르시아|2018|  LG|0.339| 50|183| 27| 62|  9|  0|  8| 95| 34|  5|  0|  9|  8| 25|  3|0.519|0.383|  9|   177cm/93kg|1985년 04월 12일|내야수(우투우타)|          쿠바 Ciego de Avila...|           null|0.902|\n",
      "|        1|     강경학|2011|한화|  0.0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0.0|  0.0|  1|   180cm/72kg|1992년 08월 11일|내야수(우투좌타)|광주대성초-광주동성중-광주동성고|      10000만원|  0.0|\n",
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+------------------+\n",
      "|batter_id|batter_name|year|team|  avg|  G| AB|  R|  H| 2B| 3B| HR| TB|RBI| SB| CS| BB|HBP| SO|GDP| SLG|  OBP|  E|height/weight|       year_born|        position|                          career|starting_salary|               OPS|\n",
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+------------------+\n",
      "|        0|   가르시아|2018|  LG|0.350|  7| 20|  1|  7|  1|  0|  1| 11|  5|  0|  0|  2|  0|  3|  1|0.55|0.409|  1|   177cm/93kg|1985년 04월 12일|내야수(우투우타)|          쿠바 Ciego de Avila...|           null|0.9590000000000001|\n",
      "|        1|     강경학|2011|한화|0.000|  4|  2|  2|  0|  0|  0|  0|  0|  1|  0|  1|  0|  2|  1|  0| 0.0|  0.5|  0|   180cm/72kg|1992년 08월 11일|내야수(우투좌타)|광주대성초-광주동성중-광주동성고|      10000만원|               0.5|\n",
      "+---------+-----------+----+----+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+----+-----+---+-------------+----------------+----------------+--------------------------------+---------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pre.csv download\n",
    "from pyspark.sql.functions import col, desc\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1t3icaDgI5KeNEwNmaWFOYGYQtdY8NOMm\",\n",
    "              \"regular.csv\")\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1g4r8tCCocVwCg6pTWaeioMdmtETYo_cf\",\n",
    "              \"pre.csv\")\n",
    "\n",
    "reg_df = sqlContext.read.csv(\"regular.csv\", header = True)\n",
    "pre_df = sqlContext.read.csv(\"pre.csv\", header = True)\n",
    "\n",
    "reg_df.show(2)\n",
    "pre_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgKsHZ8CN-fg"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 2. 생성된 각각의 DataFrame에서 **타자 이름(batter_name), 타수(AB), 안타(H)을 ``.select(col)``를 이용하여 Transformation**합니다. **단, 새로운 DataFrame의 각 column type은 string, float 또는 double로 변환합니다.** (10 point)\n",
    "\n",
    "``` # output```\n",
    "```\n",
    "★★regular DataFrame★★\n",
    "+-----------+-----+----+\n",
    "|batter_name|   AB|   H|\n",
    "+-----------+-----+----+\n",
    "|   가르시아|183.0|62.0|\n",
    "|     강경학|  1.0| 0.0|\n",
    "|     강경학| 86.0|19.0|\n",
    "+-----------+-----+----+\n",
    "only showing top 3 rows\n",
    "\n",
    "root\n",
    " |-- batter_name: string (nullable = true)\n",
    " |-- AB: double (nullable = true)\n",
    " |-- H: double (nullable = true)\n",
    "\n",
    "★★pre DataFrame★★\n",
    "+-----------+----+---+\n",
    "|batter_name|  AB|  H|\n",
    "+-----------+----+---+\n",
    "|   가르시아|20.0|7.0|\n",
    "|     강경학| 2.0|0.0|\n",
    "|     강경학| 0.0|0.0|\n",
    "+-----------+----+---+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46isUREkN-fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★regular DataFrame★★\n",
      "+-----------+-----+----+\n",
      "|batter_name|   AB|   H|\n",
      "+-----------+-----+----+\n",
      "|   가르시아|183.0|62.0|\n",
      "|     강경학|  1.0| 0.0|\n",
      "|     강경학| 86.0|19.0|\n",
      "+-----------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- AB: float (nullable = true)\n",
      " |-- H: float (nullable = true)\n",
      "\n",
      "★★pre DataFrame★★\n",
      "+-----------+----+---+\n",
      "|batter_name|  AB|  H|\n",
      "+-----------+----+---+\n",
      "|   가르시아|20.0|7.0|\n",
      "|     강경학| 2.0|0.0|\n",
      "|     강경학| 0.0|0.0|\n",
      "+-----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-2 답안 작성 \n",
    "reg_task2 = reg_df.select(reg_df.batter_name.cast(\"string\"),reg_df.AB.cast(\"float\"),reg_df.H.cast(\"float\")) \n",
    "pre_task2 = pre_df.select(pre_df.batter_name.cast(\"string\"),pre_df.AB.cast(\"float\"),pre_df.H.cast(\"float\")) \n",
    "#.select()를 사용해 새로운 DataFrame으로 Transform하고, batter_name , AB, H를 각각 string형 float형, float형으로 변환한다.\n",
    "# 형 변환 시 cast(\"type\")을 사용해 형변환을 한다.\n",
    "\n",
    "# output\n",
    "print(\"★★regular DataFrame★★\")\n",
    "reg_task2.show(3)\n",
    "reg_task2.printSchema()\n",
    "print(\"★★pre DataFrame★★\")\n",
    "pre_task2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laJkBNm0N-fi"
   },
   "source": [
    "**task**\n",
    "- 3. task2에서 생성된 각각의 DataFrame에 ``groupby``, ``agg``, ``udf`` 또는 ``임의의 함수를 자유롭게 적용``하여 **역대 정규 시즌과 시범경기의 평균 타율을 구한 후 선수 이름(column 1), 평균 타율(column 2)로 구성된 DataFrame으로 Transformation 합니다(regular, pre 모두). 단, 각각의 DataFrame의 batter_name(선수 이름)과 avg(평균 타율) column의 이름을 regular/pre_batter, regular/pre_avg로 변경할 것(``alias`` 또는 ``withColumnRenamed`` 사용),** (10 point)\n",
    "\n",
    "``` # output ```\n",
    "```\n",
    "★★regular DataFrame★★\n",
    "+--------------+-------------------+\n",
    "|regular_batter|        regular_avg|\n",
    "+--------------+-------------------+\n",
    "|        김하성| 0.2879359095193214|\n",
    "|        도태훈|0.20454545454545456|\n",
    "|        이종범| 0.2965346534653465|\n",
    "+--------------+-------------------+\n",
    "only showing top 3 rows\n",
    "\n",
    "★★pre DataFrame★★\n",
    "+----------+-------------------+\n",
    "|pre_batter|            pre_avg|\n",
    "+----------+-------------------+\n",
    "|    김하성|0.19327731092436976|\n",
    "|    도태훈|0.22727272727272727|\n",
    "|    정상호|0.22674418604651161|\n",
    "+----------+-------------------+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsilB-cmN-fj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★regular DataFrame★★\n",
      "+--------------+-------------------+\n",
      "|regular_batter|        regular_avg|\n",
      "+--------------+-------------------+\n",
      "|        김하성| 0.2879359095193214|\n",
      "|        도태훈|0.20454545454545456|\n",
      "|        이종범| 0.2965346534653465|\n",
      "+--------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "★★pre DataFrame★★\n",
      "+----------+-------------------+\n",
      "|pre_batter|            pre_avg|\n",
      "+----------+-------------------+\n",
      "|    김하성|0.19327731092436976|\n",
      "|    도태훈|0.22727272727272727|\n",
      "|    정상호|0.22674418604651161|\n",
      "+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-3 답안 작성\n",
    "from pyspark.sql.functions import udf\n",
    "def avgHit(AB_sum, H_sum):\n",
    "    if AB_sum == 0:\n",
    "        return 0\n",
    "    else:   \n",
    "        avg = H_sum/AB_sum\n",
    "    return avg\n",
    "# 타수를 안타로 나눠 평균타율을 구하는 함수 작성, 0인 값이 있을 수 있어 타수가 0이면 안타도 0이기 때문에 타수가 0이면 0을 리턴하는 예외처리를 함\n",
    "    \n",
    "avgHit_udf = udf(avgHit) # 파이썬 함수를 udf함수로 등록\n",
    "reg_task3 = reg_task2.groupby(\"batter_name\").agg({\"AB\":\"sum\", \"H\":\"sum\"}).select(col(\"batter_name\").alias(\"regular_batter\"),\n",
    "                                                                                 avgHit_udf(\"sum(AB)\",\"sum(H)\").alias(\"regular_avg\"))                                                                                \n",
    "pre_task3 = pre_task2.groupby(\"batter_name\").agg({\"AB\":\"sum\", \"H\":\"sum\"}).select(col(\"batter_name\").alias(\"pre_batter\"),\n",
    "                                                                                 avgHit_udf(\"sum(AB)\",\"sum(H)\").alias(\"pre_avg\")) \n",
    "#타자(batter_name)을 gropby를 사용해 묶고, agg를 사용해 타수(AB)와 안타(H)의 합을 계산 후 selet로 위에서 작성한 avgHit_udf함수에 \n",
    "#적용하여 colume이 batter_name인 것과 sum(AB), sum(H)인 것을 뽑아내고, \n",
    "#alias함수를 사용해 batter_name을 regular/pre_batter로, sum(AB),sum(H)를 regular/pre_avg로 재명명 한다.\n",
    "# output\n",
    "print(\"★★regular DataFrame★★\")\n",
    "reg_task3.show(3)\n",
    "print(\"★★pre DataFrame★★\")\n",
    "pre_task3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44A79sdTN-fn"
   },
   "source": [
    "**task**\n",
    "- 4. task3의 각 DataFrame(regular, pre)을 **batter_name**을 기준으로 ``join``하여,**``역대 정규시즌 평균 타율이 역대 시범경기 평균 타율보다 높은``, 선수 이름(column 1)과 해당 선수의 역대 정규시즌 평균타율(column 2)로 구성된 DataFrame으로 Transformation 합니다.**(10 point)\n",
    "\n",
    "``` # Output ```\n",
    "```\n",
    "★★regular and pre joined★★\n",
    "+--------------+-------------------+\n",
    "|regular_batter|        regular_avg|\n",
    "+--------------+-------------------+\n",
    "|        김하성| 0.2879359095193214|\n",
    "|        정상호|0.24957264957264957|\n",
    "|      스크럭스| 0.2771855010660981|\n",
    "|        지석훈| 0.2260519247985676|\n",
    "|        박경수| 0.2566744730679157|\n",
    "|        박정음|0.27440633245382584|\n",
    "|        박병호| 0.2893900889453621|\n",
    "+--------------+-------------------+\n",
    "only showing top 7 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0NUTfL0N-fn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★regular and pre joined★★\n",
      "+--------------+-------------------+\n",
      "|regular_batter|        regular_avg|\n",
      "+--------------+-------------------+\n",
      "|        김하성| 0.2879359095193214|\n",
      "|        정상호|0.24957264957264957|\n",
      "|      스크럭스| 0.2771855010660981|\n",
      "|        지석훈| 0.2260519247985676|\n",
      "|        박경수| 0.2566744730679157|\n",
      "|        박정음|0.27440633245382584|\n",
      "|        박병호| 0.2893900889453621|\n",
      "+--------------+-------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-4 답안 작성\n",
    "joinEx = reg_task3[\"regular_batter\"] == pre_task3[\"pre_batter\"] #joinexpresion\n",
    "mask = task4[\"regular_avg\"] > task4[\"pre_avg\"]\n",
    "task4 = reg_task3.join(pre_task3, joinEx)[mask].select(col(\"regular_batter\"),col(\"regular_avg\"))\n",
    "# reg_task3와 pre_task3를 regular_battar와 pre_battar의 이름이 같은(같은 key를 가진)것을 기준으로 조인 한 후 regular_avg가 pre_avg보다\n",
    "# 큰 경우를 추출 한 후 select를 이용해 ragular_battat와 regular_avg를 추출한다.\n",
    "# output\n",
    "print(\"★★regular and pre joined★★\")\n",
    "task4.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7c_lPNmN-fp"
   },
   "source": [
    "**task**\n",
    "- 5. task4에서 생성된 DataFrame에 ``내림차순``을 적용하여 **상위 10명의 선수의 이름과 역대 정규시즌 평균타율 출력합니다. 단, column 명을 출력 예시와 같게 변경할 것.** (10 point)\n",
    "\n",
    "``` # Output ```\n",
    "```\n",
    "+---------+-----------------------+\n",
    "|선수 이름|역대 정규시즌 평균 타율|\n",
    "+---------+-----------------------+\n",
    "|   장승현|    0.38461538461538464|\n",
    "|   전병우|    0.36363636363636365|\n",
    "|   이정후|    0.33827893175074186|\n",
    "|   박건우|    0.33410538506079906|\n",
    "|   김태진|     0.3333333333333333|\n",
    "|   구자욱|    0.33191489361702126|\n",
    "|   손아섭|    0.32515082171832743|\n",
    "|   김태균|     0.3247439180537772|\n",
    "|   박민우|    0.32383536861148804|\n",
    "|   김현수|     0.3226377517149812|\n",
    "+---------+-----------------------+\n",
    "only showing top 10 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hR8UJiDwN-fp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+\n",
      "|선수 이름|역대 정규시즌 평균 타율|\n",
      "+---------+-----------------------+\n",
      "|   장승현|    0.38461538461538464|\n",
      "|   전병우|    0.36363636363636365|\n",
      "|   이정후|    0.33827893175074186|\n",
      "|   박건우|    0.33410538506079906|\n",
      "|   김태진|     0.3333333333333333|\n",
      "|   구자욱|    0.33191489361702126|\n",
      "|   손아섭|    0.32515082171832743|\n",
      "|   김태균|     0.3247439180537772|\n",
      "|   박민우|    0.32383536861148804|\n",
      "|   김현수|     0.3226377517149812|\n",
      "+---------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-5 답안 작성\n",
    "task5 = task4.sort(desc('regular_avg')) # take4의 Dataframe의 regular_avg를 내림차순으로 정렬한다.\n",
    "task5 = task5.select(col(\"regular_batter\").alias(\"선수 이름\"),col(\"regular_avg\").alias(\"역대 정규시즌 평균 타율\"))\n",
    "#그런 후 alias를 사용하여 colume의 이름을 재명명한다.\n",
    "# output\n",
    "task5.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYFlyqkSN-fr"
   },
   "source": [
    "## *수고! ㅋ*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6NM8xq_7hVTC",
    "2qq_MA5ChVTQ",
    "l0w1mntDhVTJ"
   ],
   "name": "HW5_upload_V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
