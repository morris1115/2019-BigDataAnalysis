{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2VUCcpqmv7Y"
   },
   "source": [
    "# DataFrame 마무리 & Covariance Matrix(1)\n",
    "- - -\n",
    "\n",
    "#### 1. Performing a `Map` command in DataFrame\n",
    "* In order to perform a map on a dataframe, you first need to transform it into an RDD!\n",
    "* Not the recommended way. Better to use built-in sparkSQL functions\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2. Covariance Matrix\n",
    "\n",
    "* Calculating the mean of Sample Vectors\n",
    "\n",
    "* Outer product of sample vectors\n",
    "\n",
    "* Covariance Matrix\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zi0a3IDGWkc_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYVwZ8pCmv7a"
   },
   "source": [
    "### 1. Performing a `Map` command in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMUkFBYmmv7b"
   },
   "source": [
    "#### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rey9WZEmv7b",
    "outputId": "d4d5f2c3-872c-4efe-f617-128c557c75f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x7fb614eb8ac8>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sI4sev3LWnRX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GoogleDriveDownloader in /opt/conda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install GoogleDriveDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aI2e3ctxmv7e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1hAHV6vC6FvVgrYnoN-lR-IfH488-H121 into ./NY.tgz... Done.\n",
      "NY.parquet/\n",
      "NY.parquet/_SUCCESS\n",
      "NY.parquet/part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#### 예제 파일 다운로드\n",
    "from os.path import exists\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "if exists(\"./NY.tgz\"):\n",
    "    !rm -rf ./NY.tgz\n",
    "if exists(\"./NY.parquet\"):\n",
    "    !rm -rf ./NY.parquet\n",
    "    \n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = './NY.tgz')\n",
    "!tar -xzvf NY.tgz\n",
    "df = sqlContext.read.load(\"NY.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUNu1g5Tmv7f",
    "outputId": "0cb953c2-9f95-4951-f4f6-e00b731f0013",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----+--------------------+------------------+-----------------+------------------+------------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|         latitude|         longitude|         elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+------------------+-----------------+------------------+------------------+-----+-----------------+\n",
      "|USC00307659|   PRCP_s20|1917|[BF 4E B2 4E A4 4...|213.64700317382812|42.79999923706055| -74.5999984741211|249.89999389648438|   NY|SHARON SPRINGS 1N|\n",
      "|USC00305310|   SNOW_s20|1979|[00 7E 00 7E 00 7...| 81.07869720458984|41.46030044555664|-74.44889831542969|213.39999389648438|   NY|  MIDDLETOWN 2 NW|\n",
      "|USC00300023|   SNWD_s20|1915|[68 5B 7A 5B 8C 5...| 296.1679992675781|42.10139846801758| -77.2343978881836|             304.5|   NY|          ADDISON|\n",
      "+-----------+-----------+----+--------------------+------------------+-----------------+------------------+------------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema()와 sample를 이용한 데이터 확인\n",
    "df.printSchema()\n",
    "# sample 사용법 참조\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample\n",
    "df.sample(False, 0.01).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kodkuAVSmv7h"
   },
   "source": [
    "* DataFrame to RDD : `[DataFrame].rdd`, 각각의 요소가 `Row`인 RDD 생성\n",
    "\n",
    "* RDD to DataFrame : `sqlContext.createDataFrame([RDD], schema)`\n",
    "\n",
    "단, RDD에서 DataFrame으로 변환시, schema를 꼭 정의해줘야 한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GAcp0FCmv7h",
    "outputId": "c35586f2-9ba1-4f7e-808c-02c86a15a7cb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Station='USC00304102', Measurement='TOBS_s20', Year=1998, Values=bytearray(b\"\\xfe\\xd3\\x06\\xd4\\x0e\\xd4\\x18\\xd4 \\xd4)\\xd42\\xd4<\\xd4D\\xd4N\\xd4W\\xd4b\\xd4k\\xd4u\\xd4}\\xd4\\x87\\xd4\\x8f\\xd4\\x96\\xd4\\x9d\\xd4\\xa6\\xd4\\xae\\xd4\\xb7\\xd4\\xbe\\xd4\\xc4\\xd4\\xcc\\xd4\\xd3\\xd4\\xd7\\xd4\\xda\\xd4\\xdb\\xd4\\xdb\\xd4\\xdb\\xd4\\xdc\\xd4\\xdc\\xd4\\xdc\\xd4\\xd7\\xd4\\xd2\\xd4\\xcd\\xd4\\xc6\\xd4\\xbf\\xd4\\xb5\\xd4\\xa9\\xd4\\x9d\\xd4\\x92\\xd4\\x87\\xd4|\\xd4p\\xd4c\\xd4U\\xd4E\\xd47\\xd4&\\xd4\\x14\\xd4\\x05\\xd4\\xeb\\xd3\\xcc\\xd3\\xaa\\xd3\\x85\\xd3`\\xd3:\\xd3\\x14\\xd3\\xee\\xd2\\xc7\\xd2\\x9c\\xd2t\\xd2H\\xd2\\x1c\\xd2\\xf1\\xd1\\xc5\\xd1\\x98\\xd1i\\xd1;\\xd1\\t\\xd1\\xd4\\xd0\\xa0\\xd0l\\xd06\\xd0\\xfc\\xcf\\x86\\xcf\\x14\\xcf\\x9e\\xce-\\xce\\xbb\\xcdE\\xcd\\xcf\\xcc[\\xcc\\xc5\\xcb\\xc3\\xca\\xb7\\xc9\\xb7\\xc8k\\xc7R\\xc5\\xa3\\xc28\\xbds9ZA\\xd2D\\xf0F\\x80H\\x8eI\\x8fJ\\x8fKIL\\xcaLKM\\xceMLN\\xcdNMO\\xcfO\\'PgP\\xa6P\\xe7P\\'QhQ\\xa8Q\\xe8Q\\'ReR\\xa3R\\xe6R\\'SjS\\xa7S\\xe4S\\x11T1TNTjT\\x86T\\xa2T\\xbdT\\xd9T\\xf4T\\x10U*UBU[UsU\\x8bU\\xa2U\\xbaU\\xd3U\\xebU\\x05V\\x1eV8VRVlV\\x84V\\x9dV\\xb5V\\xceV\\xe7V\\x00W\\x17W-WBWYWqW\\x87W\\x9eW\\xb4W\\xcaW\\xdfW\\xf3W\\x03X\\x0eX\\x17X X)X2X:XAXHXOXUX\\\\XbXhXlXqXuXzX~X\\x82X\\x84X\\x87X\\x89X\\x8bX\\x8dX\\x8eX\\x90X\\x90X\\x90X\\x91X\\x90X\\x91X\\x90X\\x8fX\\x8eX\\x8dX\\x8cX\\x8aX\\x89X\\x87X\\x85X\\x84X\\x82X\\x7fX}XzXwXuXrXnXkXgXcX`X\\\\XXXTXPXKXFX@X;X5X/X)X$X\\x1eX\\x18X\\x12X\\x0bX\\x04X\\xfbW\\xecW\\xdcW\\xcdW\\xbdW\\xadW\\x9cW\\x8bWwWeWSW>W+W\\x18W\\x03W\\xefV\\xdaV\\xc5V\\xb0V\\x9bV\\x85VoVXV?V(V\\x11V\\xf9U\\xe2U\\xcbU\\xb2U\\x99U\\x81UgUNU4U\\x19U\\xffT\\xe5T\\xccT\\xb2T\\x98T|TaTFT,T\\x12T\\xf0S\\xbbS\\x87SSS S\\xe8R\\xb4R\\x84ROR\\x19R\\xe6Q\\xb4Q\\x80QNQ\\x1aQ\\xe7P\\xb7P\\x86PUP%P\\xf0O\\x98O?O\\xe1N\\x85N*N\\xd1M}M/M\\xdaL\\x8bL0L\\xbcK\\x12K{J\\xdeI4I}H\\xadGXF\\nE/Cr@\\xa1:2\\xb80\\xc07\\xc33\\xc5\\xd0\\xc6<\\xc8\\x19\\xc9\\xfd\\xc9\\xee\\xca\\xd9\\xcbd\\xcc\\xdb\\xcc\\\\\\xcd\\xe2\\xcdj\\xce\\xf6\\xce\\x88\\xcf\\r\\xd0Z\\xd0\\xa7\\xd0\\xf2\\xd0=\\xd1\\x84\\xd1\\xd0\\xd1\\x1d\\xd2g\\xd2\\xb4\\xd2\\xfd\\xd2C\\xd3\\x8a\\xd3\\xcb\\xd3\\x06\\xd4\\'\\xd4H\\xd4f\\xd4\\x82\\xd4\\x9d\\xd4\\xb6\\xd4\"), dist_coast=294.364013671875, latitude=43.755001068115234, longitude=-74.26920318603516, elevation=506.0, state='NY', name='INDIAN LAKE 2SW')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_rdd = df.rdd.takeSample(False, 1)\n",
    "some_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RC9wok3kmv7j"
   },
   "source": [
    "* Lec 6의 예제\n",
    "\n",
    "    1. 주어진 DataFrame에서 `Year가 1900 미만인 경우 '19th'`, `2000 미만인 경우 '20th'`, `2010 미만인 경우 '21st'`, `모두 아닐 경우 'possibly_bad_data'로 값을 치환`하여라\n",
    "\n",
    " 여기서 `map`에 들어가는 `input`이 무엇인지 반드시 숙지하여야 한다.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSSFoeotmv7k",
    "outputId": "f2713cb4-283f-47fd-a597-5c859bfb5a74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20th', '20th', '20th', '20th', '20th']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_century(row):\n",
    "    if row.Year < 1900:\n",
    "        return \"19th\"\n",
    "    elif row.Year < 2000:\n",
    "        return \"20th\"\n",
    "    elif row.Year < 2010:\n",
    "        return \"21st\"\n",
    "    else:\n",
    "        return \"possibly_bad_data\"\n",
    "    \n",
    "df.rdd.map(find_century).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzFXzrhmmv7l"
   },
   "source": [
    "* Lec 6의 예제\n",
    "\n",
    "    2. 주어진 DataFrame에서 각각의 요소 중 `longitude`와 `latitude`를 추출하여 (longitude, latitude)의 형태로 값을 출력하여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2KNPUc0mv7m",
    "outputId": "7aa18389-8ca7-4356-bcca-9848b2433dee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125),\n",
       " (-77.71330261230469, 42.57080078125)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda row : (row.longitude, row.latitude)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAz482zDmv7n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OuM3jhrmv7p"
   },
   "source": [
    "### Excercise 1 - RDD function in DataFrame (60 point)\n",
    "\n",
    "- - -\n",
    "task 별로 ``위에서 사용한 DataFrame(df)``에서 ``RDD``로 변환 후 ``RDD function``을 적용하여 해결합니다\n",
    "\n",
    "(task 당 20 point)\n",
    "\n",
    "---\n",
    "\n",
    "**task**\n",
    "\n",
    "* 1 : ``df``에서 ``name``별 가장 최근 ``Year``를 **map과 reduce, 또는 reduceByKey 등**을 활용하여 구한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 2 : ``df``에서 ``Year``가 ``2000 이상``인 결과에 대해, ``name``별 ``Year``, ``dist_coast``, ``elevation``의 평균을 구하고 ``name``를 기준으로 정렬(Z->A)한 후, take(5)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 3 : ``df``에서 ``Measurement`` 별 `Values`의 1일부터 10일까지(``np.frombuffer(row.Values[:20], dtype='float16')`` 또는 ``np.frombuffer(row.Values, dtype = 'float16')[:10])의 합이 가장 큰 ``Year``와 ``그 값을 구한 후``, ``Measurement``를 기준으로 정렬(A->Z)합니다. 마지막으로 collect를 하여 출력합니다.(20 point) \n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNu7gzmpmv7p"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 1 : ``df``에서 ``name``별 가장 최근 ``Year``를 **map과 reduce, 또는 reduceByKey 등**을 활용하여 구한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "```\n",
    "#task1 output\n",
    "[('DANSVILLE MUNI AP', 2013),\n",
    " ('BRIDGEHAMPTON', 2013),\n",
    " ('MIDDLETOWN 2 NW', 2011),\n",
    " ('BERLIN 5 S', 2000),\n",
    " ('ELMIRA CORNING RGNL AP', 2013),\n",
    " ('UNADILLA 2 N', 2013),\n",
    " ('SUFFERN 2 E', 1955),\n",
    " ('ROXBURY', 1972),\n",
    " ('LOWVILLE', 2013),\n",
    " ('GABRIELS', 1978)]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5Qtu4q6mv7p",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CANTON 4 SE', 2013),\n",
       " ('VICTOR 2NW', 2012),\n",
       " ('UNADILLA 2 N', 2013),\n",
       " ('LOWVILLE', 2013),\n",
       " ('ADAMS CTR', 1950),\n",
       " ('HONEOYE', 2013),\n",
       " ('EAST SIDNEY', 2013),\n",
       " ('NEW YORK BENSONHURST', 1953),\n",
       " ('DELANSON 2NE', 2013),\n",
       " ('CANAJOHARIE', 1976)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-1 답 작성\n",
    "df.rdd.map(lambda row :(row.name, row.Year)).reduceByKey(lambda x,y : x if x > y else y).take(10)\n",
    "# df의 데이터를 rdd의 row의 튜플로 받아 reduceByKey를 하여 key 별로 value 값을 비교하여 큰 값을 추출."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2xTPOidmv7r"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 2 : ``df``에서 ``Year``가 ``2000 이상``인 결과에 대해, ``name``별 ``Year``, ``dist_coast``, ``elevation``의 평균을 구하고 ``name``를 기준으로 정렬(Z->A)한 후, take(10)을 통하여 출력합니다.(20 point)\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "```\n",
    "# task2 output\n",
    "[('YOUNGSTOWN 2 NE', (2008.5, 476.80999755859375, 85.30000305175781)),\n",
    " ('YORKTOWN HEIGHTS 1W',\n",
    "  (2006.421686746988, 28.945999145507812, 204.1999969482422)),\n",
    " ('WINDHAM 3 E', (2004.8387096774193, 147.28700256347656, 512.0999755859375)),\n",
    " ('WILLSBORO 1 N',\n",
    "  (2005.360655737705, 255.24200439453125, 54.900001525878906)),\n",
    " ('WHITNEY POINT DAM', (2009.2826086956522, 235.35800170898438, 317.0))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-nU-RqQmv7r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YOUNGSTOWN 2 NE', (2008.5, 476.80999755859375, 85.30000305175781)),\n",
       " ('YORKTOWN HEIGHTS 1W',\n",
       "  (2006.922077922078, 28.945999145507812, 204.1999969482422)),\n",
       " ('WINDHAM 3 E', (2005.357142857143, 147.28700256347656, 512.0999755859375)),\n",
       " ('WILLSBORO 1 N',\n",
       "  (2005.8392857142858, 255.24200439453125, 54.900001525878906)),\n",
       " ('WHITNEY POINT DAM', (2009.2826086956522, 235.35800170898438, 317.0)),\n",
       " ('WHITEHALL', (2006.922077922078, 220.0189971923828, 36.29999923706055)),\n",
       " ('WESTHAMPTN GABRESKI AP',\n",
       "  (2006.7317073170732, 4.3188700675964355, 20.399999618530273)),\n",
       " ('WESTFIELD 2 SSE', (2002.0, 418.3210144042969, 215.5)),\n",
       " ('WESTCHESTER CO AP', (2006.5348837209303, 7.586299896240234, 115.5)),\n",
       " ('WEST POINT', (2006.5, 47.6151008605957, 97.5))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-2 답 작성\n",
    "df.rdd.map(lambda row : (row.name,(row.Year, row.dist_coast, row.elevation,1))).filter(lambda x : x[1][0] > 2000).reduceByKey(lambda x,y : tuple([(x[i] + y[i]) for i in range(len(x))])).map(lambda x:(x[0],tuple([x[1][0]/x[1][3], x[1][1]/x[1][3], x[1][2]/x[1][3]]))).sortByKey(False).take(10)\n",
    "# df rdd를 map하여 row로 받아 name key를 가지고 value로 Year와 dist_coast, elevaion,1을 가지는 새로운 rdd를 생성 후 \n",
    "#filter를 통해 새로 변형된 df rdd의 1번째의 0번째 인덱스가 2000 인상인 값만 추출 한 후, \n",
    "#reduceByKey를 통해 키 값을 기준으로 value들을 다 더한 후 다시 map을 사용해 각 인자들을 하나씩 접근하여 인덱싱 하여 평균을 구하고, \n",
    "#sortBykey를 flase로 지정하여 내림차순으로 정렬한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjP5V4xymv7w"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 3 : ``df``에서 ``Measurement`` 별 `Values`의 1일부터 10일까지(``np.frombuffer(row.Values[:20], dtype='float16')`` 또는 ``np.frombuffer(row.Values, dtype = 'float16')[:10])의 합이 가장 큰 ``Year``와 ``그 값을 구한 후``, ``Measurement``를 기준으로 정렬(A->Z)합니다. 마지막으로 collect를 하여 출력합니다.(20 point) \n",
    "\n",
    "<br>\n",
    "\n",
    "**★ DataFrame이 아닌 RDD로 작업할 것**\n",
    "\n",
    "★★여기서 ``Values``는 ``bytearray`` type입니다. ★★\n",
    "\n",
    "``numpy``의 **frombuffer**를 이용하여 ``float16``으로 바꿉니다.\n",
    "자세한 사용법은 [여기](https://docs.scipy.org/doc/numpy/reference/generated/numpy.frombuffer.html)를 참고합니다\n",
    "\n",
    "★★또한, ``numpy``의 ``nansum``을 이용하여 값이 **nan**이 아닌 Values의 합을 구합니다.★★\n",
    "\n",
    "```\n",
    "# task3 output\n",
    "[('PRCP', (12824.0, 1946)),\n",
    " ('PRCP_s20', (4264.0, 1983)),\n",
    " ('SNOW', (2912.0, 1954)),\n",
    " ('SNOW_s20', (4140.0, 1895)),\n",
    " ('SNWD', (16590.0, 1970)),\n",
    " ('SNWD_s20', (12320.0, 1976)),\n",
    " ('TMAX', (2216.0, 1897)),\n",
    " ('TMAX_s20', (2050.0, 1897)),\n",
    " ('TMIN', (623.0, 2007)),\n",
    " ('TMIN_s20', (21360.0, 1987)),\n",
    " ('TOBS', (1000.0, 1998)),\n",
    " ('TOBS_s20', (969.5, 1992))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wBSUHVbmv7w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRCP', (12824.0, 1946)),\n",
       " ('PRCP_s20', (4264.0, 1983)),\n",
       " ('SNOW', (2912.0, 1954)),\n",
       " ('SNOW_s20', (4140.0, 1895)),\n",
       " ('SNWD', (16590.0, 1970)),\n",
       " ('SNWD_s20', (12320.0, 1976)),\n",
       " ('TMAX', (2216.0, 1897)),\n",
       " ('TMAX_s20', (2050.0, 1897)),\n",
       " ('TMIN', (623.0, 2007)),\n",
       " ('TMIN_s20', (21360.0, 1987)),\n",
       " ('TOBS', (1000.0, 1998)),\n",
       " ('TOBS_s20', (969.5, 1992))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df.rdd.map(lambda row : (row.Measurement,(np.nansum(np.frombuffer(row.Values[:20], dtype='float16')),row.Year))).reduceByKey(lambda x,y : x if x > y else y).sortByKey().collect()\n",
    "# df을 rdd로 사용하며 map을 사용하여 새로운 rdd형태를 만드는데 Measurement를 key값으로 value는 Values와 Year이다.\n",
    "# 여기서 Value의 데이터 타입이 bytearry이기 때문에 numpy의 frombuffer를 사용하여 float16으로 변환한다.\n",
    "# 여기서 np.nansum을 사용하여 Values의 값이 nan을 제외하고 합한다.\n",
    "# 그런 후 reduceByKey를 사용하여 합의 값이 가장 큰 값을 Key를 기준으로 뽑아 낸 후 sortByKey를 적용하여 오름차순으로 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7An8T1Nq2ltD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0Z5dZeD2ltF"
   },
   "source": [
    "### 2. Spark와 Numpy를 사용하여 Covariance Matrix 구해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt_7LRtgBY_a"
   },
   "source": [
    "#### (1) numpy 기초 \n",
    "아래 참고자료를 활용하여 numpy 기초 학습\n",
    "\n",
    "* 참고자료 \n",
    "  * [참고자료 1](http://taewan.kim/post/numpy_cheat_sheet/)\n",
    "  * [참고자료 2](https://docs.scipy.org/doc/numpy/user/quickstart.html)\n",
    "  * [참고자료 3](https://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays)\n",
    "  * [참고자료 4](https://doorbw.tistory.com/171)\n",
    "  * [참고자료 5](https://datascienceschool.net/view-notebook/17608f897087478bbeac096438c716f6/)\n",
    "\n",
    "* 위 자료에서중에서도 \n",
    "  * ndarray 생성법\n",
    "  * vector, matrix 연산\n",
    "  * 인덱싱 (slicing)\n",
    "  * 행렬 합치기 (vstack, dstack, hstack)\n",
    "  * sum, mean\n",
    "  * np.nan 자료형\n",
    "  * reshape\n",
    "  * matmul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9N7GWN5m0aj"
   },
   "source": [
    "#### (2)  Calculating the mean of Sample Vectors \n",
    "다음 벡터들을 샘플 벡터로 가정합니다.\n",
    "* $n$ 은 샘플의 수 이고\n",
    "* $d$는 각 데이터 벡터의 길이 입니다 (예를 들어서 날씨데이터의 경우 $d=365$)\n",
    "$$\n",
    "\\mathbf{x}_i = \\left[\\begin{array}{cccc}\n",
    "x_{i1} & x_{i2}& \\ldots & x_{id}, \n",
    "\\end{array}\\right], \\quad i=1,\\ldots, n\n",
    "$$\n",
    "Sample vector 들의 mean (평균) 벡터 $\\bar{\\mathbf{x}}$는 다음과 같이 구합니다 \n",
    "$$\n",
    "\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yl5QnvQZ4yvX"
   },
   "outputs": [],
   "source": [
    "# 평균 벡터 구하는 문제 (sum, mean 활용 둘다 해도 괜찮습니다))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELHX1CFB2ltL",
    "outputId": "4e7c71c5-3d6c-4c7e-99dd-c5837af7b920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 sample의 값 :  [1 2 3] [4 5 6] [7 8 9]\n",
      "sample vector sum :  [12 15 18]\n",
      "sample vector mean :  [4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "# in python\n",
    "import numpy as np\n",
    "\n",
    "sample1 = np.array([1,2,3])\n",
    "sample2 = np.array([4,5,6])\n",
    "sample3 = np.array([7,8,9])\n",
    "print(\"각 sample의 값 : \", sample1, sample2, sample3)\n",
    "print(\"sample vector sum : \", sample1 + sample2 + sample3)\n",
    "print(\"sample vector mean : \", (sample1 + sample2 + sample3) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMYlw8JX9f7u",
    "outputId": "0be9a8c8-9ba1-462b-fc10-ef3b58a89d60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 sample의 값 :  [array([1, 2, 3]), array([4, 5, 6]), array([7, 8, 9])]\n",
      "sample vector sum :  [12 15 18]\n",
      "sample vector mean :  [4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "# in spark\n",
    "vector_list = sc.parallelize([np.array([1,2,3]),np.array([4,5,6]),np.array([7,8,9])])\n",
    "print(\"각 sample의 값 : \", vector_list.collect())\n",
    "print(\"sample vector sum : \", vector_list.reduce(lambda ndarr1, ndarr2 : ndarr1 + ndarr2))\n",
    "print(\"sample vector mean : \",\n",
    "      vector_list.reduce(lambda ndarr1, ndarr2 : ndarr1 + ndarr2) / vector_list.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vVD-9dp2ltR"
   },
   "source": [
    "### Excercise 2 - Calculating the mean of Sample Vectors (40 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "\n",
    "**위의 두 데이터는 모두 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "- - -\n",
    "\n",
    "**task**\n",
    "\n",
    "- 1. filter를 사용하여 팀 이름이 ``두산``인 선수에 대해, ``(batter_id, np.array([G,R,H,RBI,BB]))``의 형태로 Key/Value RDD를 생성합니다. (20 point)\n",
    "\n",
    "    1. G R H RBI BB의 경우 초기 설정값이 ``stirng``.  이 값들을 ``float64``으로 변경할 것. ex) ``np.array([1,2,3], dtype = 'float64')``\n",
    "    2. ★ 각 값이 `' '`일 경우, 0 으로 변경할 것. \n",
    "    3. ``map``에서 바로 적용 또는 그러한 함수를 작성\n",
    "    \n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. (20 point)\n",
    "    1. ``reduceByKey``를 사용하여 ``batter_id``(Key)가 동일한 선수의 ``G, R, H, RBI, BB``(Value)를 각각 더해준 후 ``batter_id``(Key)를 기준으로 ``sortByKey``를 적용합니다. 그 후, ``map``을 사용하여 ``G, R, H, RBI, BB``(Value)만 선택 후 새로운 RDD로 만듭니다.\n",
    "    2. 위에서 생성된 RDD에 대해, 중복되지 않는 sample의 수를 ``count``를 이용하여 구합니다.\n",
    "    3. ``reduce``를 이용하여 ``G, R, H, RBI, BB``(Value)를 모두 더한 후 위에서 구한 sample의 수(`count`)로 나누어서 sample vector의 평균을 구합니다.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sE2zQXh92ltS"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1b_L-rJYJC9Oqga0fQ2zh2M763CTM8jzR\", \"regular.csv\")\n",
    "regular = sc.textFile(\"./regular.csv\").map(lambda x : x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsNIE2482ltW"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 1. filter를 사용하여 팀 이름이 ``두산``인 선수에 대해, ``(batter_id, np.array([G,R,H,RBI,BB]))``의 형태로 Key/Value RDD를 생성합니다. (20 point)\n",
    "\n",
    "    1. G R H RBI BB의 경우 초기 설정값이 ``stirng``.  이 값들을 ``float64``으로 변경할 것. ex) ``np.array([1,2,3], dtype = 'float64')``\n",
    "    2. ★ 각 값이 `' '`일 경우, 0 으로 변경할 것. \n",
    "    3. ``map``에서 바로 적용 또는 그러한 함수를 작성\n",
    "    \n",
    "```\n",
    "#output\n",
    "[(7, array([1., 0., 0., 0., 0.])),\n",
    " (7, array([64., 16., 21., 11.,  6.])),\n",
    " (7, array([93., 30., 40., 25., 16.])),\n",
    " (7, array([6., 3., 3., 3., 2.])),\n",
    " (7, array([40., 14., 17.,  5., 10.])),\n",
    " (7, array([48., 12., 24., 10., 18.])),\n",
    " (17, array([16.,  1.,  1.,  1.,  0.])),\n",
    " (17, array([32.,  2.,  3.,  0.,  0.])),\n",
    " (17, array([16.,  2.,  2.,  0.,  0.])),\n",
    " (17, array([116.,  38.,  85.,  29.,  24.]))]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7', array([1., 0., 0., 0., 0.])),\n",
       " ('7', array([64., 16., 21., 11.,  6.])),\n",
       " ('7', array([93., 30., 40., 25., 16.])),\n",
       " ('7', array([6., 3., 3., 3., 2.])),\n",
       " ('7', array([40., 14., 17.,  5., 10.])),\n",
       " ('7', array([48., 12., 24., 10., 18.])),\n",
       " ('17', array([16.,  1.,  1.,  1.,  0.])),\n",
       " ('17', array([32.,  2.,  3.,  0.,  0.])),\n",
       " ('17', array([16.,  2.,  2.,  0.,  0.])),\n",
       " ('17', array([116.,  38.,  85.,  29.,  24.]))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "task1 = regular.filter(lambda x : x[3] == '두산').map(lambda x : (x[0],(np.array([x[5],x[7],x[8],x[13],x[16]],dtype = 'float64'))))\n",
    "task1.take(10)\n",
    "# regular 데이터셋에서 filter를 사용하여 데이터 column의 3번째 인자인 team명이 '두산'인 값만 추출한다.\n",
    "# 그런 후 map을 사용하여 데이터셋의 0번째(batter_id)를 key로 가지고, 5번째(G),7번째(R),8번째(H),13번째(RBI),16번째(BB)를 value로 가지는 RDD를 생성한다.\n",
    "# 여기서 numpy의 array의 형태로 출력는데 dataType이 string인 값을 float64로 변경한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVX2WOiY2lte"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 2. (20 point)\n",
    "    1. ``reduceByKey``를 사용하여 ``batter_id``(Key)가 동일한 선수의 ``G, R, H, RBI, BB``(Value)를 각각 더해준 후 ``batter_id``(Key)를 기준으로 ``sortByKey``를 적용합니다. 그 후, ``map``을 사용하여 ``G, R, H, RBI, BB``(Value)만 선택 후 새로운 RDD로 만듭니다.\n",
    "    2. 위에서 생성된 RDD에 대해, 중복되지 않는 sample의 수를 ``count``를 이용하여 구합니다.\n",
    "    3. ``reduce``를 이용하여 ``G, R, H, RBI, BB``(Value)를 모두 더한 후 위에서 구한 sample의 수(`count`)로 나누어서 sample vector의 평균을 구합니다.\n",
    "\n",
    "```\n",
    "# output\n",
    "[G, R, H, RBI, BB] mean :  [427.8846  186.32692 344.05768 178.0577  129.48077]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[G, R, H, RBI, BB] mean :  [427.88461538 186.32692308 344.05769231 178.05769231 129.48076923]\n"
     ]
    }
   ],
   "source": [
    "task2 = task1.reduceByKey(lambda x,y : x + y).sortByKey().map(lambda x : x[1])\n",
    "task2_mean_vector = task2.reduce(lambda x,y : x+y) / task2.count() # 52\n",
    "print(\"[G, R, H, RBI, BB] mean : \", task2_mean_vector)\n",
    "\n",
    "# 위의 task에서 생성한 rdd를 reduceByKey를 적용하여 같은 key를 가진 value들을 합하고, sortByKey를 적용하여 오름차순으로 정렬 한 후,\n",
    "# map을 사용하여 기존 RDD 리스트의 1번째 인자들로만 구성된 새로운 RDD를 생성한후 task2에 저장한다.\n",
    "# task2에 count()함수를 적용시켜보면 52가 나오고,새로 생성된 task2 RDD를 reduce하여 값들을 모두 합한 후 count()한 값으로 나누어 준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43q9xB_nvllT"
   },
   "source": [
    "# 기말고사가 다가온다.ㅋ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW7_upload_V4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
